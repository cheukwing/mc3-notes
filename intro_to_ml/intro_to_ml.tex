\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsmath} \usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{ {./imgs/} }

\setlength{\parindent}{0pt}

\title{Introduction to Machine Learning (CO395)}
\author{Michael Tsang}

\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{theo}{Theorem}
\newtheorem{lem}{Lemma}

\begin{document}

\maketitle
\section{Machine Learning}
\textit{The field of machine learning is concerned with the question of how to construct computer programs that automatically improve with experience.}

\subsection{Supervised Learning}
\begin{defn}
Learning an unknown mapping $f(x_i) = y_i$ from training data, given in the form of input  and output pairs $D = \{ (x_i, y_i) \}^N_{i = 1}$.
\end{defn}

\begin{itemize}
  \item $D$ is the \textbf{training set}, $N$ is the number of training examples/samples/data points. 
  \item The $x_i$ are in the \textbf{feature space} $\mathcal{X}$ and the $y_i \in \mathcal{Y}$ are in the \textbf{label space}.
  \item In the simplest setting, each training input $x_i$ is a $D$-dimensional vector of numbers, these are called \textbf{features}, \textbf{attributes}, or \textbf{covariates}.
  \item In general, $x_i$ could be a complex structured object, such as an image or sentence.
\end{itemize}

\subsubsection{Spaces}
\begin{figure}[h]
  \caption{Spaces before the \textit{Deep Learning Era}.}
  \includegraphics[scale=0.2]{spacesbefore}
  \centering
\end{figure}

\begin{figure}[h]
  \caption{Spaces after the \textit{Deep Learning Era}.}
  \includegraphics[scale=0.2]{spacesafter}
  \centering
\end{figure}

\subsubsection{Label Space}
There are three main types of \textbf{Label Spaces}:
\begin{itemize}
  \item \textbf{Categorical} (also called class) - $y_i$ is a categorical or nominal variable from some finite set $y_i \in \{1, \ldots, C \}$ (e.g.\ ``Apple'', ``Banana''.).
    The supervised learning task is known as \textbf{classification} or \textbf{pattern recognition}.
  \item \textbf{Real-valued Scalar} - e.g.\ credit score, the task is known as \textbf{regression} or \textbf{function approximation}.
  \item \textbf{Ordinal Regression} - this occurs when the label space has some natural ordering which can be approximated as a regression problem that is subsequently discretised, e.g.\ school grades.
\end{itemize}

\subsection{Unsupervised Learning}
\begin{defn}
Discovering an underlying, hidden, or latent structure within the data $x$.
\end{defn}

The aim is to find a structure that explains the data in a more efficient way.
We can think of this as reducing the amount of bits to store the important features of the data, akin to lossy data compression. \\

This is possible in broadly two ways:
\begin{itemize}
  \item \textbf{Dimensionality reduction} - reducing the dimensions in the data.
  \item \textbf{Clustering} - assigning the data to automatically defined categorical labels.
\end{itemize}

\subsection{Reinforcement Learning}
\begin{defn}
Finding which action to take in order to maximise the received rewards.
\end{defn}

The main differences are:
\begin{itemize}
  \item The best or correct solution is not given to the agent, only a reward signal.
  \item Feedback is delayed.
  \item Time matters, data is sequential.
  \item Agent's decisions affect the subsequent received data.
\end{itemize}

An example of this is in a moving robot.

\subsection{Main Problems in Machine Learning}
\begin{itemize}
  \item \textbf{Classification} - predicting the right label for an unknown sample.
  \item \textbf{Regression} - approximating an unknown function.
  \item \textbf{Clustering} - grouping data in such a way that data points in the same group (cluster) are more similar to each other than to those in other clusters.
  \item \textbf{Dimensionality Reduction} - reducing the dimensionality of the observed data.
  \item \textbf{Density Estimation} - estimating an unobservable underlying probability density function based on observed data.
  \item \textbf{Policy Search} - finding which action an agent should take, depending on its current state, to maximise the received rewards.
\end{itemize}

\section{Instance Based Learning}
We need large amounts of data to make accurate predictions; selecting the right features and representation is crucial.

\subsection{$k$-Nearest Neighbours}
One way of implementing a classifier is to consider the $k$-nearest neighbours in the feature space of the current instance, and \textbf{assign the class in the majority}.

We define the nearest neighbours of a query instance $x_q$ in terms of the Euclidean distance ($L2$-norm):
\[
  d(x_i, x_q) = \sqrt{\sum_g (a_g(x_i) - a_g(x_q))^2}
\]
where the instances $x_i$ belong to the dataset, and all instances are described with a set of $g = [1, \ldots, p]$ features $a_g$.

Alternatively we could use different definitions of distance:
\begin{itemize}
  \item Manhattan ($L1$-norm)
    \[
      d(x_i, x_q) = \sum_g \lvert a_g(x_i) - a_g(x_q) \rvert
    \]
  \item Chebyshev ($L^\infty$-norm)
    \[
      d(x_i, x_q) = \max \lvert a_g(x_i) - a_g(x_q) \rvert
    \]
\end{itemize}

\subsubsection{Choice of $k$}
\begin{itemize}
  \item Small $k$ - Good resolution of class borderlines, but sensitive to noise.
  \item Large $k$ - Bad resolution of class borderlines, but robust to noise.
\end{itemize}

We choose a value of $k$ with a validation dataset.

While the $k$-NN algorithm is powerful, finding the nearest neighbours can be slow if the dataset is large.
Approaches to improve this include: $k$-d trees; Locality-Sensitive hashing with hash tables; or generating prototypes with Learning Vector Quantisation.

\subsection{Distance-Weighted $k$-NN Algorithm}
We assign a weight $w_r$ to each neighbour $x_r$ of the query instance $x_q$ based on the distance $d(x_r, x_q)$: \textbf{nearer neighbours have greater weight}.

Any measure favouring the votes of nearby neighbours works:
\begin{itemize}
  \item Inverse of the distance
    \[
      w_r = \frac{1}{d(x_r, x_q)}
    \]
  \item Gaussian distribution
    \[
      w_r = \frac{1}{\sqrt{2 \pi}} \exp \left(\frac{-d(x_r, x_q)^2}{2}\right)
    \]
\end{itemize}

The value of $k$ is less important as distance examples will have small weight and not greatly affect the classification.

If $k = n$, where $n$ is the total number of instances, the algorithm is a \textbf{global method}.
Otherwise if $k < n$, it is a \textbf{local method}, only takes into account some of the instances. \\

Since classification is based on a weighted combination of all $k$-NN, then the impact of noise is smoothed out - the distance-weighted $k$-NN is \textbf{robust to noisy training data}.

Since the distance is based on all features of each instance, irrelevant features may cause instances that belong in the \textbf{same class to be distant from one another}. \\

To remedy this, we \textbf{weight each feature} differently when calculating the distance.

\subsection{Curse of Dimensionality}
\begin{figure}[h]
  \caption{The relationship between average distance and dimensions.}
  \includegraphics[scale=0.3]{curseofdim}
  \centering
\end{figure}

As the dimensions of the feature space increases, the distance to the nearest neighbours increases.

\subsection{$k$-NN for Regression}
Instead of using the class in the majority, we consider the \textbf{value} in the majority.

In distance-weighted $k$-NN, the prediction of the algorithm is the \textbf{weighted average value} of the $k$-NN.

This is also known as \textit{Locally Weighted Regression}.

\subsection{Lazy Learning}
\begin{defn}
Data is stored, but generalising beyond this is postponed until an explicit request is made.
\end{defn}

\begin{enumerate}
  \item Search the memory for similar instances.
  \item Retrieve the related solutions.
  \item Adapt solutions to the current instance.
  \item Assign estimated solution to the current instance.
\end{enumerate}

An example of this is the $k$-NN algorithm.

\begin{itemize}
  \item A \textbf{different approximation to the target function} is constructed for each query instance.
  \item The collection of \textbf{less complex local approximations} represents a complex target function, where the problem domain could be incomplete.
  \item Large space requirement to store the entire training dataset, with long query time.
  \item Most useful for large datasets with few attributes.
\end{itemize}

\subsection{Eager Learning}
\begin{defn}
  A general, explicit description of the target function is constructed, based on the provided training examples.
\end{defn}

Examples of this are in artifical neural networks and decision trees.

\begin{itemize}
  \item The \textbf{same approximation to the target function} is used, which must be learned based on training examples, and before input queries are observed.
  \item Better memory efficiency, and usually low query time.
  \item Deals better with noise.
  \item Generally unable to provide good local approximations in the target function.
\end{itemize}

\section{Decision Trees}
Decision tree learning is a method for approximating discrete classification functions in a tree-based representation.
A learned decision tree can be represented as a set of if-else rules.

The algorithm is as follows:
\begin{enumerate}
  \item Search for a split point using a statistical test of each attribute to determine how well it classifies the training examples when considered alone.
  \item Split the dataset according to the split point.
  \item Repeat on each of the created subsets.
\end{enumerate}

We choose the split point best on \textbf{information gain}, by quantifying the reduction of the information entropy.

\subsection{Information Entropy}
\begin{defn}
  \textbf{Entropy} is a measure of the uncertainty of a random variable, or the averaged quantity of information required to fully define a random state.
\end{defn}

\[
  H(V) = - \sum_k P(v_k) \log_2 (P(v_k))
\]

For a probability density function $f(x)$, we can define the \textbf{continuous entropy}:
\[
  H(V) = - \int_x f(x) \log_2 (f(x))
\]
$f(x)$ is usually unknown, but can be approximated with density estimation algorithms.

\subsection{Information Gain}
The most informative question in the decision tree maximises the information gain - the difference between the initial entropy and the weighted average entropy of the produce subsets.

\[
  G(q) = H(\text{dataset}) - \left( \frac{\lvert \text{subsetA} \rvert}{\lvert \text{dataset} \rvert} H(\text{subsetA}) + \frac{\lvert \text{subsetB} \rvert}{\lvert \text{dataset} \rvert} H(\text{subsetB}) \right)
\]
where $\lvert \text{dataset} \rvert  = \lvert \text{subsetA} \rvert + \lvert \text{subsetB} \rvert$.

\subsection{Types of Input}
\begin{itemize}
  \item \textbf{Ordered values} (like real-values):
    \begin{enumerate}
      \item For each feature, start by sorting the values of the attribute.
      \item Consider only split points that are between two examples in sorted order that have different classifications.
    \end{enumerate}
  \item \textbf{Symbolic values}:
    \begin{enumerate}
      \item Search for the most informative feature.
      \item Create as many branches as there are different values for this feature.
    \end{enumerate}
\end{itemize}

\subsection{Overfitting}
\begin{figure}[htb!]
  \centering
  \caption{ML algorithm with differing amounts of fit to the data.}
  \includegraphics[scale=0.3]{overfitted}
\end{figure}

We can deal with overfitting by splitting the dataset into two parts: \textbf{training} and \textbf{validation}.

We use the training dataset to train the model and the validation dataset to stop the training when performance on the validation test degrades.

\subsubsection{Pruning}
The most common approach with decision trees is \textbf{pruning}.
\begin{enumerate}
  \item Go through all the nodes that are connected to leaves.
  \item Check if the accuracy on the validation dataset would increase if this node is turned into a leaf.
\end{enumerate}
This may need to be done recursively.

\subsection{Regression}
Decision trees can be used for classification and regression, though the former is much more common.
We call a decision tree used for regression a \textbf{regression tree}.

Instead of a single class in each leaf, there is a linear function of some subset of the numerical features.
This requires that the learning algorithm decide when to stop splitting and begin applying linear regression over the features.

\section{Evaluating Hypotheses}
The ultimate goal in machine learning is to create models or algorithms that can generalize to unknown data.

To ensure meaningful evaluation, we use a \textbf{test dataset}.
This should \textbf{not} be used to train the model - it simulates unknown data.

\subsection{Parameter Tuning}
We want to find good parameter values that work with unknown data.

\textbf{Incorrect} approaches:
\begin{itemize}
  \item Try different values on the training dataset, select the best according to the accuracy on the training dataset.
    \begin{itemize}
      \item May not generalize well.
    \end{itemize}
  \item Try different values on the training dataset, select the best according to the accuracy on the test dataset.
    \begin{itemize}
      \item The test dataset is now part of the training process.
      \item Cannot evaluate how the algorithm generalizes to unknown data.
    \end{itemize}
\end{itemize}

\subsubsection{The Correct Approach}
\begin{itemize}
  \item Split dataset into \textbf{training}, \textbf{validation}, and \textbf{test}.
  \item Usually $60\%/20\%/20\%$ split.
  \item Try different values on the training dataset, select the best according to accuracy on the validation dataset, and perform the final evaluation on the test dataset.
  \item Parameter choice takes into account how the model would generalise, and final evaluation considers only unknown data.
\end{itemize}

\subsection{Holdout Method}
\begin{enumerate}
  \item Keep the classifier that leads to maximum performance on the validation set - \textbf{parameter optimization/tuning}.
  \item Parameters are fixed, can choose to use the model trained on the training dataset, or combine the training and validation datasets to train a new model (with the same parameters).
  \item Finally, evaluate on the test dataset.
\end{enumerate}

Once parameters are fixed and we have an estimation for the performance of the model, we retrain it on the \textbf{entire dataset} (including test) as we want the best model.

\subsection{Cross Validation Method}
With a small sample size, a better alternative is to use \textbf{cross validation}.

\begin{enumerate}
  \item Divide the dataset into $k$ (usually $10$) folds.
  \item Use $k - 1$ for training and validation, $1$ for testing:
    \begin{itemize}
      \item Test data between different iterations should never overlap.  
      \item Training/validation and test data in the same iteration should never overlap.
    \end{itemize}
  \item In each iteration, the error on the test set is estimated.
  \item We calculate the average of the $k$ errors: $\frac{1}{N}\sum_{i = 1}{N} e_i$.
\end{enumerate}

\subsubsection{Parameter Tuning}
\begin{enumerate}
  \item Divide the $k-1$ folds into training and validation folds: $k - 2$ for training, $1$ for validation.
  \item Train on the training set, optimize parameters on the validation set, test on the test set.
\end{enumerate}
We can only estimate the test set performance, we evaluate how our implementation generalizes on unknown test sets.

We know nothing about the optimal parameters - we find a different set of optimal parameters in each fold.

Once we know the average accuracy of the model, we use the entire dataset to estimate the optimal set of parameters:
\begin{enumerate}
  \item $k-1$ folds for training, $1$ for validation.
  \item For each parameter, run the $k$ fold cross validation.
  \item Select the parameters that result in the best average performance over all $k$ left out folds.
\end{enumerate}

\subsection{Metrics}
\subsubsection{Confusion Matrix}
\begin{center}
  \begin{tabular}{c c c}
    & Class 1 Predicted & Class 2 Predicted \\
    Class 1 Actual & \textbf{TP} & \textbf{FN} \\
    Class 2 Actual & \textbf{FP} & \textbf{TN}
  \end{tabular}
\end{center}
We use the confusion matrix to visualize the performance of an algorithm and to calculate other performance measures.

The table highlights the risk of each prediction - in some cases it may be more important to have low false negatives than low false positives.

\subsubsection{Classification Rate}
\[
  \frac{TP + TN}{TP + TN + FP + FN}
\]
\begin{itemize}
  \item The number of correctly classified examples divided by the total number of examples.
  \item $\text{Classification Error } = 1 - \text{ Classification Rate}$.
  \item The probability of a correct classification.
\end{itemize}

\subsubsection{Recall}
\[
  \frac{TP}{TP + FN}
\]
\begin{itemize}
  \item The number of correctly classified positive examples divided by the total number of positive examples.
  \item High recall - class is correctly recognized.
  \item Probability of a positive example being classified as such.
\end{itemize}

\subsubsection{Precision}
\[
  \frac{TP}{TP + FN} 
\]
\begin{itemize}
  \item The number of correctly classified positive examples divided by the total number of predicted positive examples.
  \item High precision - a positively labeled example is positive.
  \item Probability that a positively classified example is positive.
\end{itemize}

\subsubsection{Precision and Recall}
\begin{itemize}
  \item High recall, low precision - most positive examples are correctly recognized but there are a lot of false positives.
  \item Low recall, high precision - lots of missed positive examples, but those predicted as positive are positive.
\end{itemize}

\subsubsection{Unweighted Average Recall}
The mean of the recall of each class.

\subsubsection{F-Measure/Score}
\begin{align*}
  F_\alpha &= (1 + \alpha^2)\frac{\text{Precision } \times \text{ Recall}}{(\alpha^2 \times \text{ Precision}) + \text{ Recall}} \\
  F_1 &= 2\frac{\text{Precision } \times \text{ Recall}}{\text{Precision } + \text{ Recall}}
\end{align*}

\subsection{Imbalanced Test Set}
In a balanced dataset, the number of examples in each class are similar - all measures result in similar performance.

In an imbalanced dataset, the classes are not equally represented:
\begin{itemize}
  \item CR is affected a lot by the majority class.
  \item Precision for minority classes are significantly affected - examples belonging to the majority are misclassified.
  \item UAR can detect that one class is misclassified, but gives no information on false positives.
  \item F1 can be useful, but also affected by the class imbalance problem.
\end{itemize}

It is important to look at the confusion matrix.

\subsubsection{Solutions to the Imbalance Problem}
\begin{itemize}
  \item We can divide each value in the confusion matrix by the total number of examples.
    \begin{itemize}
      \item These would be the results if we had the same number of examples and the performance of the classifier remained the same.
      \item There is no guarantee the performance would remain the same.
    \end{itemize}
  \item Upsample the minority class.
  \item Downsample the majoirty class.
\end{itemize}

\subsection{Overfitting}
\begin{defn}
  \textbf{Overfitting} gives good performance on the training data but poor generalization to other data.
\end{defn}

\begin{defn}
  \textbf{Underfitting} gives poor performance on the training data and poor generalization to other data.
\end{defn}

Overfitting could occur when:
\begin{itemize}
  \item Learning performed for too long.
  \item Examples in the training set not representative of all possible situations.
  \item Model too complex.
\end{itemize}

We can fight overfitting by:
\begin{itemize}
  \item Stopping training earlier - use validation set to know when.
  \item Get more data.
  \item Use the right level of complexity - validation set.
\end{itemize}

\subsection{Comparing Algorithms}
\subsubsection{Error}
\begin{defn}
  The \textbf{true error} of the model $h$ is the probability that it will misclassify a randomly drawn example $x$ from distribution $D$:
  \[
    \text{error}_D(h) = P(f(x) \neq h(x)) 
  \]
\end{defn}

\begin{defn}
  The \textbf{sample error} of the model $h$ based on a data sample $S$ is:
  \[
    \text{error}_S(h) = \frac{1}{N} \sum_{x \in S} \delta(f(x), h(x))
  \]
  where:
  \begin{align*}
    N &= \text{ number of samples} \\
    \delta(f(x), h(x)) &= 1 \text{ if } f(x) \neq h(x) \\
    \delta(f(x), h(x)) &= 0 \text{ if } f(x) = h(x)
  \end{align*}
\end{defn}

We want to know the true error but can only measure the sample error.

\subsubsection{Confidence Interval}
\begin{defn}
  A $N\%$ \textbf{confidence interval} for some parameter $p$ is an interval that is expected with probability $N\%$ to contain $p$.
\end{defn}

Given a sample $S$, we can say with $N\%$ confidence, the true error lies in the interval:
\[
\text{error}_S(h) \pm Z_N \sqrt{\frac{\text{error}_S(h) \times (1 - \text{ error}_S(h))}{n}}
\]

\subsubsection{Comparing Two Algorithms}
We run a statistical test to tell us if there is a difference between two distributions of classification errors, in order to see which is better.

Our tests return a \textbf{p-value}.
We make our \textbf{null hypothesis} the case that there is no performance difference between the two algorithms tested.
If the performance difference is \textbf{statistically significant} ($p < 0.05$), then we reject the null hypothesis.

A higher p-value does not mean the algorithms are similar, only that there is no observable statistical difference.

Other notes:
\begin{itemize}
  \item The evaluation of a model represents what performance can be expected when applying the model on a similar data distribution.
  \item If the model is stochastic, we should replicate the training several times to know the distribution of the performance.
\end{itemize}

\end{document}
