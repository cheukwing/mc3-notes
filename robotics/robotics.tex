\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{ {./imgs/} }

\setlength{\parindent}{0pt}

\title{Robotics (CO333)}
\author{Michael Tsang}

\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{theo}{Theorem}
\newtheorem{lem}{Lemma}

\begin{document}

\maketitle

\section{Robot Motion}
A mobile robot can move and sense, and must process information to link these two.

What are the possible goals of a robot locomotion system?
\begin{itemize}
  \item Speed and/or acceleration of movement.
  \item Precision of positioning (repeatability).
  \item Flexibility and robustness in different conditions.
  \item Efficiency.
\end{itemize}

Robots could move in water, air, land, or even space.
We focus on wheeled robots which move on fairly flat surfaces.

\subsection{Motion and Coordinate Frames}
We define two coordinate frames: a \textbf{world frame} $W$ anchored in the world and a \textbf{robot frame} $R$ which is carried by and stays fixed relative to the robot at all times.

We are interested in the robot's \textbf{location}: the transformation between $W$ and $R$.

\subsection{Degrees of Motion Freedom}
A rigid body which translates and rotates along a:
\begin{itemize}
  \item 1D path has 1 degree of freedom (DOF): 1 translational.
  \item 2D plane has 3 DOF: 2 translational, 1 rotational.
  \item 3D volume has 6 DOF: 3 translational, 3 rotational.
\end{itemize}

A \textbf{holonomic robot} is one which is able to move instantaneously in any direction in the space of its degrees of freedom.

Although holonomic robots exist, they need many motors or unusual designs, and are often impractical.

\subsection{Wheel Configurations}
\begin{itemize}
  \item Rack and Pinion.
  \item Differential Drive.
  \item Skid-Steer.
  \item Synchro Drive.
\end{itemize}
These are standard non-holonomic configurations and are simple, reliable, robust mechanisms suitable for robots which essentially move in a plane.

Some more exotic non-holonomic configurations are:
\begin{itemize}
  \item Segway - good height with small footprint and high acceleration; self balancing.
  \item Mars Rover - wheels on stalks to tackle large obstacles.
\end{itemize}

\subsection{Differential Drive}
\begin{itemize}
  \item Two motors, one per wheel.
  \item Steer by setting different speeds on each wheel.
  \item Wheels run at equal speeds for straight-line motion.
  \item Wheels run at equal and opposite speeds to turn on the spot.
  \item Other combinations of speeds allow circular arcs.
\end{itemize}

\subsection{Circular Path of a Differential Drive Robot}
\begin{figure}[h]
  \caption{Differential Drive Robot}
  \includegraphics[scale=0.4]{ddrobot}
  \centering
\end{figure}

Let the wheel velocities of the left and right wheels respectively be $v_L$ and $v_R$.
These are linear velocities of the wheels over the ground: $v_X = r_X\omega_X$, where $r_X$ is the radius of the $X$ wheel and $\omega_X$ is its angular velocity.

The width between the wheels is $W$.

\begin{itemize}
  \item Straight line motion if $v_L = v_R$.
  \item Turns on the spot if $v_L = - v_R$.
\end{itemize}

To find radius $R$ of a curved path, consider a period of motion $\Delta t$ where the robot moves along a circular arc through angle $\Delta \theta$.
\begin{itemize}
  \item Left wheel: $\text{distance moved } = v_L \Delta t$; $\text{radius of arc } = R - \frac{W}{2}$.
  \item Right wheel: $\text{distance moved } = v_R \Delta t$; $\text{radius of arc } = R + \frac{W}{2}$.
  \item Both wheel arcs subtend the same angle so:
    \[
      \Delta \theta = \frac{v_L \Delta t}{R - \frac{W}{2}} = \frac{v_R \Delta t}{R + \frac{W}{2}}
    \]
    \[
      \implies \frac{W}{2}(v_L + v_R) = R(v_R - v_L)
    \]
    \begin{align*}
      \implies R &= \frac{W(v_R + v_L)}{2(v_R - v_L)} & \Delta \theta &= \frac{(v_R - v_L)\Delta t}{W}
    \end{align*}
\end{itemize}

\subsection{Rack and Pinion Drive (Car/Tricycle)}
\begin{figure}[h]
  \caption{Rack and Pinion Drive layouts.}
  \includegraphics[scale=0.4]{piniondrive}
  \centering
\end{figure}

\begin{itemize}
  \item Two motors: one to drive, one to steer.
  \item Cannot normally turn on the spot.
  \item With fixed speed and steering angle, will follow a circular path.
  \item With four wheels, need rear differential and variable (``Ackerman'') linkage for steering wheels.
\end{itemize}

\subsection{Circular Path of a Car-Like Tricycle Robot}
\begin{figure}[h]
  \caption{Tricycle Robot.}
  \includegraphics[scale=0.5]{tricycle}
  \centering
\end{figure}

Single steerable and drivable wheel at back, front wheels are free running.

Assuming no sideways wheel slip, we intersect the axes of the front and back wheels to forma right-angle triangle:
\[
  R = \frac{L}{\tan s}
\]

The radius of the path the rear wheels moves is:
\[
  R_d = \frac{L}{\sin s}
\]

In time $\Delta t$, the distance along its circular arc moved by the wheel is $v\Delta t$, so the angle $\Delta \theta$ through which the robot rotates is:
\[
  \Delta \theta = \frac{v\Delta t}{R_d} = \frac{v\Delta t \sin s}{L}
\]

\subsection{Gearing}
\begin{figure}[h]
  \caption{Gears of a DC motor.}
  \includegraphics[scale=0.5]{gearing}
  \centering
\end{figure}

DC motors offer high speed and low torque, so gearing is nearly always required to drive a robot.

If Gear 1 is driven with torque $t_1$, it exerts tangential force:
\[
  F = \frac{t_1}{r_1}
\]

on Gear 2; the torque in Gear 2 is therefore:
\[
  t_2 = r_2F = \frac{r_2}{r_1}t_1
\]

The change in angular velocity between Gear 1 and Gear 2 is calcualted by considering velocity at the point where they meet:
\[
  v = \omega_1 r_1 = \omega_2 r_2
\]
\[
  \implies \omega_2 = \frac{r_1}{r_2}\omega_1
\]
\begin{itemize}
  \item When a small gear drives a bigger gear, the second gear has higher torque and lower angular velocity in proportion to the ratio of teeth.
  \item Gears can be chained together to achieve compound effects.
\end{itemize}

\subsection{Motor Control - Open Loop}
\begin{figure}[h]
  \caption{Open loop system.}
  \includegraphics[scale=0.4]{openloop}
  \centering
\end{figure}

Let $P$ be a Single-Input-Single-Output (SISO) dynamic system, it is described by:
\begin{itemize}
  \item An input $u$ - here: voltage $V$, or corresponding Pulse Width Modulation (PWM) value.
  \item Internal states $x$, whose dynamics follow differential equations - here: $x = [x_1, x_2]^\top = [\omega, \varphi]^\top$, with rotation speed $\omega$ and angle $\varphi$.
  \item An output $y$ as a function of $x$ - here: angle $\varphi$, $y = x_2$.
\end{itemize}

Qualitative open-loop response on input (voltage) step: input $V$ leads to output motor angle $\phi$, which changes with time.

\subsection{Motor Control - Closed Loop}
\begin{figure}[h]
  \caption{Closed loop system.}
  \includegraphics[scale=0.4]{closedloop}
  \centering
\end{figure}

Let $C$ be a controller possibly with internal states:
\begin{itemize}
  \item $r$ is a reference (desired) output.
  \item $e$ is the e  rror between reference and actual output.
\end{itemize}

The controller runs at a high frequency, at each iteration it checks the current error and calculates a control value to the motor which aims to reduce the error.

This is simple: a binary on or off.

\subsection{General Motor Control - PID}
Proportional-Integral-Differential (PID), a controller:
\[
  C : u(t) = k_p \exp(t) + k_i \int_{t_0}^{t} \exp(\tau)d_\tau + k_d \frac{d\exp(t)}{dt}
\]
where
\begin{itemize}
  \item $k_p$ - proportional gain, reduces the error.
  \item $k_i$ - integral gain, removes steady-state error.
  \item $k_d$ - differential gain, can reduce settling time.
\end{itemize}

A simple (heuristic) tuning rule is the \textbf{Ziegler-Nichols}:
\begin{itemize}
  \item Set $k_i$ and $k_d$ to zero.
  \item Increase $k_p$ until the system starts oscillating with Period $P_u$ (in seconds) - remember this gain as $k_u$.
  \item Set $k_p = 0.6k_u, k_i = 2k_p/P_u, \text{and } k_d = k_pP_u/8$.
\end{itemize}

\subsection{Motor Control - Additional Tweaks}
\begin{itemize}
  \item Reference filtering - respect physical limits already in reference.
  \item Anti-Reset-Windup - stops integrating the effor for the I-part, when $u$ is at its physical limit.
  \item Dead-band compensation - add offset to $u$ to compensate friction.
  \item Feed-forward controller - $C_f : u_f(t) = k_f \frac{dr(t)}{dt}$, reduces ``work'' for the feedback controller
\end{itemize}

\subsection{Mapping Wheel Rotation Speed to Velocity}
In principle, we could emasure the radius of each wheel $r_W$ to turn angular into linear motion.
However in practice, due to hard to model factors, it is better to calibrate empirically, i.e.\ work out the scaling between the motor reference angle and the distance travelled over the ground via experiments.

\subsection{Motion and State on a 2D Plane}
We can define a \textbf{state vector} $\textbf{x}$:
\[
  \textbf{x} = 
  \begin{pmatrix}
    x \\ y \\ \theta
  \end{pmatrix}
\]
\begin{itemize}
  \item $x$ and $y$ specify the location of the pre-define ``robot centre'' point in the world frame.
  \item $\theta$ specifies the rotation angle between the two coordinate frames (the angle between the $x^W$ and $x^R$ axes).
  \item At the origin, $x = y = \theta = 0$.
\end{itemize}

\subsection{Integrating Motion in 2D}
2D motion on a plane: three degrees of position freedom $(x, y, \theta)$, with $-\pi < \theta \leq \pi$.

Consider a robot which either drives ahead or turns on the spot:
\begin{itemize}
  \item During straight-line period of motion of distance $D$:
    \[
      \begin{pmatrix}
        x_{\text{new}} \\
        y_{\text{new}} \\
        \theta_{\text{new}} 
      \end{pmatrix}
      =
      \begin{pmatrix}
        x + D\cos\theta \\
        y + D\sin\theta \\
        \theta 
      \end{pmatrix}
    \]
  \item During a pure rotation of angle $\alpha$:
    \[
      \begin{pmatrix}
        x_{\text{new}} \\
        y_{\text{new}} \\
        \theta_{\text{new}} 
      \end{pmatrix}
      =
      \begin{pmatrix}
        x \\
        y \\
        \theta + \alpha
      \end{pmatrix}
    \]
\end{itemize}

\subsection{Integrating Circular Motion Estimates in 2D}
\begin{figure}[h]
  \caption{Circular Motion estimation.}
  \includegraphics[scale=0.4]{circularmotionestimate}
  \centering
\end{figure}

\[
  \begin{pmatrix}
    x_{\text{new}} \\
    y_{\text{new}} \\
    \theta_{\text{new}} 
  \end{pmatrix}
  =
  \begin{pmatrix}
    x + R(\sin(\Delta \theta + \theta) - \sin \theta) \\
    y - R(\cos(\Delta \theta + \theta) - \cos \theta) \\
    \theta + \Delta \theta
  \end{pmatrix}
\]

\subsection{Position-Based Path Planning}
Assuming that robot has localisation and knows where it is relative to a fixed coordinate frame, then \textbf{position-based path planning} allows it to move in precise way along a sequence of pre-defined waypoints.
Paths of various curved shapes could be planning, aiming to optimise criteria (e.g.\ time or power).

We assume that:
\begin{itemize}
  \item Movements are composed by straight-line segments separated by turns on the spot.
  \item Total distance travelled is minimised - it always faces to turn the next waypoint and drive straight towards it.
\end{itemize}

If the current pose is $(x, y, \theta)$ and the next waypoint is $(W_x, W_y)$, it must first rotate to point towards the waypoint at vector direction:
\[
  \begin{pmatrix}
    d_x \\
    d_y
  \end{pmatrix}
  =
  \begin{pmatrix}
    W_x - x \\
    W_y - y
  \end{pmatrix}
\]
The absolute angular orientation $\alpha$ the robot must drive in is:
\[
  \alpha = \tan^{-1} \frac{d_x}{d_y}
\]

Care must be taken to ensure $\alpha$ is in the correct quadrant of $-\pi < \alpha \leq \pi$.

The angle the robot must rotate through is therefore $\beta = \alpha - \theta$.
To move as efficiently as possible, care should be taken to shift this angle by adding or subtracting $2\pi$ to ensure $-\pi < \beta \leq \pi$.

The robot should then drive forward straight for
\[
  d = \sqrt{d_x^2 + d_y^2}
\]

\section{Sensors}
\subsection{A Well Calibrated Robot}
After careful calibration, a robot should move to the desired location on average, but scatter will remain due to uncontrollable factors (e.g.\ wheel slip, rough surfaces).
Although \textbf{systematic error} may have been removed, \textbf{zero mean errors} still remain.

The errors occur incrementally: small additional movements/rotations induce slightly more potential error.

We can model the zero mean errors probabilistically, usually a Gaussian (normal) distribution.

\subsection{Uncertainty in Motion}
We modify the previous equations to acknowledge the uncertain perturbations:
\[
  \begin{pmatrix}
    x_{\text{new}} \\
    y_{\text{new}} \\
    \theta_{\text{new}} 
  \end{pmatrix}
  =
  \begin{pmatrix}
    x + (D + e)\cos\theta \\
    y + (D + e)\sin\theta \\
    \theta + f
  \end{pmatrix}
\]
\[
  \begin{pmatrix}
    x_{\text{new}} \\
    y_{\text{new}} \\
    \theta_{\text{new}} 
  \end{pmatrix}
  =
  \begin{pmatrix}
    x \\
    y \\
    \theta + \alpha + g
  \end{pmatrix}
\]

Here, $e, f, g$ are uncertainty terms with zero mean and Gaussian distribution, they model how the actual motion may deviate from the ideal trajectory.

These equations will be helpful when we probabilistically combine odometry with other sensing.

\subsection{Sensor Types}
Sensors are either \textbf{proprioceptive} (self-sensing) or \textbf{exteroceptive} (outward-looking).
\begin{itemize}
  \item Proprioceptive sensors (e.g.\ motor encoders, internal force sensors) improve a robot's sense of its own internal state and motion.
  \item Without exteroceptive sensors, a mobile robot moves blindly, it needs to:
    \begin{itemize}
      \item Localise without drift and with respect to a map.
      \item Recognise places and objects it has seen before.
      \item Map out free space, avoid obstacles.
      \item Interact with objects and people.
      \item Be aware of its environment.
    \end{itemize}
\end{itemize}

\subsection{Sensor Measurements: Proprioceptive}
\begin{itemize}
  \item Sensors gather numerical readings or \textbf{measurements}; for proprioceptive sensors, the value of the measurement $z_p$ will depend on just the state of the robot $x$:
    \[
      z_p = z_p(x)
    \]
  \item The state of a robot is a vector of variables describing its current status.
  \item More generally, a proprioceptive measurement might depend not just on the current state but also on previous states or the current rate of change of state.
\end{itemize}

\subsection{Sensor Measurements: Exteroceptive}
\begin{itemize}
  \item A measurement from an exteroceptive sensor depends on both the state of the robot $x$ and the world $y$:
    \[
      z_o = z_o(x, y)
    \]
  \item The state of the world might be parameterised in many ways, e.g.\ a list of geometric coordinates of walls or landmarks, either uncertain or perfectly known.
\end{itemize}

\subsection{Single and Multiple Value Sensors}
\begin{itemize}
  \item Touch, light and sonar sensors return a singe value within a given range.
  \item Sensors such as a camera or laser range-finder return an array of values, achieved either by scanning a single sensing element or by having an array of sensing elements.
\end{itemize}

\subsection{Touch Sensor}
\begin{itemize}
  \item Binary on/off state - no processing required.
  \item Switch open - no current flow.
  \item Switch closed - current flow.
\end{itemize}

\subsection{Light Sensor}
\begin{itemize}
  \item Detect intensity of light incident from a single forward direction, with some range of angular sensitivity.
  \item Multiple sensors pointing in different directions can guide steering behaviours.
  \item The Lego sensors can also emit their own light, which reflect off close targets.
\end{itemize}

\subsection{Sonar (Ultrasonic) Sensor}
\begin{itemize}
  \item Measures depth by emitting an ultrasonic pulse and timing interval for echo to return.
  \item Fairly accurate in one direction but can give noisy measurements in the presence of complicated shapes.
  \item Ring of sonar sensors can be used for obstacle detection.
  \item Important for underwater robots as it is the only serious option beyond very short ranges.
\end{itemize}

\subsection{External Sensing: Laser Range-Finder}
\begin{itemize}
  \item Measures depth using an active signal; commercial LADAR sensors return an array of depth measurements from a scanning beam.
  \item Very accurate measurement depth, works on most surface types.
  \item Normally scans in a 2D plane.
  \item Bulky and expensive.
\end{itemize}

\subsection{External Sensing: Vision}
\begin{itemize}
  \item Generalisation of a light sensor - measures light intensity in many directions simultaneously by directing incident light onto a sensing chip with an array of sensitive elements.
  \item Returns large, rectangular array of measurements.
  \item A single camera measures light intensity, multiple cameras or a single moving one allows 3D information processing.
  \item Low cost.
\end{itemize}

\subsection{Touch Sensors for Bump Detection}
The sensor detects when an obstacle has been hit, demanding an immediate reaction (e.g.\ evasive manoeuvre or stopping motion).

For a circular robot, we can mount touch sensors inside a ``floating skirt'' to give the ability to measure different bump directions.

\begin{itemize}
  \item Stationary object collision: explore by reversing and trying to go around.
  \item Moving object collision: follow or run away.
\end{itemize}

\subsection{Servoing}
\textbf{Servoing} is a robot control technique where control parameters are coupled directly to a sensor reading and updated regularly in a \textbf{negative feedback loop}, sometimes known as \textbf{closed loop control}.

Servoing requires high frequency update of sensor/control cycle, otherwise motion may oscillate.

\subsection{Exteroceptive Servoing}
In servoing, a control demand is set which over time aims to bring the current value of a sensor reading to agree with a desired value.

\textbf{Proportional control:} set demand proportional to negative error (difference between desired value and actual value).
\[
  -k_p(z_{\text{desired}} - z_{\text{actual}})
\]
where $k_p$ is the proportional gain constant.

\begin{itemize}
  \item \textbf{Cascade control} is where the output of one control loop is used to set the input to another, hiding the details of motor/encoder control from the upper level work with sensor control. 
  \item Problems only arise when the top level controller requests changes too rapidly, exceeding the \textbf{bandwidth} of lower level controllers.
  \item Sensors may sometimes produce garbage readings for a number of different possible reasons; it is often sensible to use a strategy to remove outliers but this also reduces the responsiveness of the system.
\end{itemize}

\subsection{Visual Servoing to Control Steering}

\begin{figure}[h]
  \caption{Steering for a robot with a tricycle or car-type wheel configuration.}
  \includegraphics[scale=0.4]{steering}
  \centering
\end{figure}

Simple steering law to collide with target:
\[
  s = k_p \alpha
\]
  
Steering law to avoid obstacle at safe radius:
\[
  s = k_p (\alpha - \sin^{-1} \frac{R}{D})
\]

\subsection{Combining: Sensing/Action Loops}
We consider each local ``servo''-like sensing-action loop as a \textbf{behaviour}, we do not model or plan.

Sense $\rightarrow$ Act.

The challenge is combining many behaviours into useful overall activity.

\subsection{Combining: World Model Approach}
\begin{itemize}
  \item Capture data, store, and manipulate using symbolic representations.
  \item Plan a sequence of actions to achieve a given goal.
  \item Execute plan.
  \item If world changes during execution, stop and re-plan.
\end{itemize}

This is powerful, but computationally expensive and complicated.

Probabilistic state inference and planning is the modern version of this, and able to cope with uncertainty in sensors.

\subsection{Probabilistic Sensor Modelling}
Robot sensing, like motion, is fundamentally uncertain.
Real sensors do not report the exact truth but a perturbed version.

Having characterised (modelled, calibrated) a sensor and understood the uncertainty in measurements, we can build a probabilistic measurement model for how it works.
This is a \textbf{likelihood function}:
\[
  p(z_o \mid x, y)
\]
which often has a Gaussian shape.

\subsection{Likelihood Functions}
A \textbf{likelihood function} fully describes a sensor's performance.

$p(z \mid v)$ is a function of both measurement variables $z$ and ground truth $v$, and can plotted as a probability surface.

\begin{figure}[h]
  \caption{Probability surfaces for a depth sensor.}
  \includegraphics[scale=0.4]{uncertainty}
  \centering
\end{figure}

\section{Probabilistic Robots}
Problem: simple sensing/action procedures can be locally effective but are limited; we need longer-term representations and consistent scene models.

Classical AI approaches (logical reasoning) fail with real-world data:
\begin{itemize}
  \item Advanced sensors do not lend themselves to straightforward analysis.
  \item All information received is uncertain.
\end{itemize}

A probabilistic approach acknowledges uncertainty and uses models to abstract useful information from data - our goal is an incrementally updated probabilistic estimate of the position of the robot relative to the map.

\subsection{Uncertainty in Robotics}
\begin{itemize}
  \item Every robot action is uncertain.
  \item Every sensor measurement is uncertain.
  \item When we combine actions adn measurements to estimate state, the state estimate will be uncertain.
\end{itemize}

Usually, we start with an uncertain state estimate, take some action, receive new sensor information, then must update the uncertain state estimate in response.

\subsection{Probabilistic Inference}
What is my state and that of the world around me?
\begin{itemize}
  \item Prior knowledge is combined with new measurements, generally modelled as a \textbf{Bayesian Network}.
  \item Series of weighted combinations of old and new measurements.
  \item \textbf{Sensor fusion} - combining data from many different sources into useful estimates.
  \item The composite state estimate can then be used to decide the next action.
\end{itemize}

\subsection{Bayesian Probabilistic Inference}
\begin{itemize}
  \item ``Bayesian'' - measure of subjective belief.
  \item Probabilities describe our state of knowledge, not randomness.
  \item \textbf{Bayes' Rule:}
    \[
      P(X \mid Z) = \frac{P(Z \mid X)P(X)}{P(Z)}
    \]
  \item $P(X)$ - \textbf{prior}; $P(Z \mid X)$ - \textbf{likelihood}; $P(X \mid Z)$ - \textbf{posterior}; ($P(Z)$ - \textbf{marginal likelihood}).
  \item We use Bayes' Rule to incrementally digest new information from sensors about a robot's state.
\end{itemize}

\subsection{Probability Distributions}
\begin{itemize}
  \item As we decrease bin size, discrete probabilistic inference generalises to large numbers of possible states, limiting to a continuous probability density function.
  \item Explicit Gaussian distributions often represent uncertainty in sensor measurements well.
  \item The Gaussian prior multiplied by the Gaussian likelihood produce a Gaussian posterior which is tighter than either.
  \item Particles are a finite set of weighted samples of the state.
  \item With a high number of particles, they can represent the shape of distribution in ambigiuous situations.
\end{itemize}

\subsection{Probabilistic Localisation}
\begin{itemize}
  \item The robot has a map of its environment in advance.
  \item The only uncertainty is the robot's position.
  \item The robot stores and updates a probability distribution representing its uncertain position estimate.
\end{itemize}

\subsection{Particle Filter}
A cloud of particles represent uncertain robot states; the more particles in a region, the higher the probability the robot is there.

A particle is a point estimate $x_i$ of the state of the robot with weight $w_i$:
\[
  x_i = 
  \begin{pmatrix}
    x_i \\
    y_i \\
    \theta_i
  \end{pmatrix}
\]

The full particle set is:
\[
  \{ x_i, w_i \} \text{ for } i = 1..N \text{ (usually N = 100)}
\]

All weights should add up to 1, if so the distribution is \textbf{normalised}:
\[
  \sum_{i = 1}^N w_i = 1
\]

Interpretation: the probability the robot is within any region of the state space is the sum of all the particles within that region.

\section{Monte Carlo Localisation (MCL)}
A cloud of weighted particles represent the uncertain position of a robot.
We can think of how MCL works in two ways:
\begin{itemize}
  \item A Bayesian probabilistic filter.
  \item ``\textbf{Survival of the fittest}'' - when a measurement is made, the likelihood of each particle is assigned according to how well it fits the measurement, we normalise and resample then allow the strong particles to reproduce, while the weak particles die out. 
\end{itemize}

\subsection{Continuous vs. Global Localisation}
\textbf{Continuous}: a tracking problem, given a good estimate of where the robot was at the last time-step and some new measurements, estimate its new position.

\textbf{Global}: the ``kidnapped robot problem'', the environment is known, but the robot's position within it is completely uncertain.

MCL can tackle both problems, the difference is in the initialisation of the particle set.

\begin{itemize}
  \item In continuous localisation, we assume a perfectly known start robot position: we set the state of all particles to be the same value, and the weights to be equal.
    \[
      x_1 = x_2 = \ldots = x_N = x_{init}
    \]
    \[
      w_1 = w_2 = \ldots = w_N = \frac{1}{N}
    \]
  \item In global localisation, we start only knowing the robot is somewhere within a certain region, the state of each particle is sampled randomly from all possible positions within that region, all weight are equal.
    \[
      x_i = Random
    \]
    \[
      w_1 = w_2 = \ldots = w_N = \frac{1}{N}
    \]
\end{itemize}

Global localisation is only practical with more than one sensor.

\subsection{Inferring an Estimate and Position-Based Navigation}
At any point, our uncertain estimate of the robot location is represented by the whole particle set.
We make a point estimate by taking the mean of all particles:
\[
  \bar{x} = \sum_{i = 1}^N w_i x_i
\]

The robot could use this to plan $A \rightarrow B$ waypoint navigation.

\subsection{Displaying a Particle Set}
\begin{figure}[h]
  \caption{Particle set representation}
  \includegraphics[scale=0.4]{particleset}
  \centering
\end{figure}

We visualise by plotting the $x$ and $y$ coordinates as a set of dots.
It is more difficult to visualise the $\theta$, but we can get the main idea from just the linear components.

\subsection{Steps in MCL/Particle Filtering}
\begin{enumerate}
  \item Motion Prediction based on Proprioceptive Sensors.
  \item Measurement Update based on Extereoceptive Sensors.
  \item Normalisation.
  \item Resampling.
\end{enumerate}

\subsection{Motion Prediction}
Uncertainty grows during blind motion.
When the robot makes a movement, the particle distribution shifts its mean position but also spreads out.

This is achieved by passing the state part of each particle through a function which has a deterministic and a random component.

During a straight-line period of motion of distance $D$:
\[
  \begin{pmatrix}
    x_{new} \\
    y_{new} \\
    \theta_{new}
  \end{pmatrix}
  =
  \begin{pmatrix}
    x + (D + e)\cos \theta \\
    y + (D + e)\sin \theta \\
    \theta + f
  \end{pmatrix}
\]

During a pure rotation of angle $\alpha$:
\[
  \begin{pmatrix}
    x_{new} \\
    y_{new} \\
    \theta_{new}
  \end{pmatrix}
  =
  \begin{pmatrix}
    x \\
    y \\
    \theta + \alpha + g
  \end{pmatrix}
\]

Here, $e, f, g$ are zero mean \textbf{noise} terms, random numbers with a Gaussian distribution which are generated for each particle, causing spread.

To set $e, f, g$, make initial estimates then adjust be looking at the particle spread over an extended motion and matching the distribution to experiments.

\subsection{Measurement Updates}
A measurement update consists of applying Bayes' Rule to each particle:
\[
  P(X \mid Z) = \frac{P(Z \mid X) P(X)}{P(Z)}
\]

When we achieve a measurement $z$, we update the weight of each particle as follows:
\[
  w_{i(new)} = P(z \mid x_i) \times w_i
\]
The denominator in Bayes' rule is a constant factor which will later be removed by normalisation so it is not calculated.

$P(z \mid x_i)$ is the likelihood of particle $i$, the probability of getting measurement $z$ given that it represents the true state.

\subsection{Likelihood Function}
The form of a likelihood function comes from a probabilistic model of the exteroceptive sensor.
It fully describes a sensor's performance.

Having calibrated a sensor and understood the uncertainty in its measurements (via repeated experiments), we can build a probabilistic measurement model for how it works:
\[
  P(z \mid x_i)
\]
where $z$ is a measurement variable and $v$ the ground truth.

This is a probability distribution (a likelihood function), often with a Gaussian shape.

\subsection{Measurement Update: How do we get the Ground Truth Value?}
\begin{figure}[h]
  \caption{Robot facing a wall.}
  \includegraphics[scale=0.4]{measurementupdate}
  \centering
\end{figure}
If a robot is at pose $(x, y, \theta)$, then its forward distance to an infinite wall passing through $(A_x, A_y)$ and $(B_x, B_y)$ is:
\[
  m = \frac{(B_y - A_y)(A_x - x) - (B_x - A_x)(A_y - y)}{(B_y - A_Y)\cos \theta - (B_x - A_x) \sin \theta}
\]

\subsection{Measurement Update: Sonar}
The world coordinates at which the forward vector from the robot meet the wall are:
\[
  \begin{pmatrix}
    x + m \cos \theta \\
    y + m \sin \theta
  \end{pmatrix}
\]
This can be used to check if the sonar should hit between the endpoint limits of the wall.

If the sonar independently hits several of these walls, then the closest is the one it will actually respond to.

\subsection{Likelihood for Sonar Update}
The likelihood should depend on the difference $z - m$: if this is small, the particle is validated, else it is weakened.

The further away the measurement is from the prediction, the less likely it is to occur - the Gaussian function is a good model for this.
\[
  p(z \mid m) \propto e^{\frac{-(z - m)^2}{2 \sigma^2_s}}
\]
where $\sigma_s$ is the standard deviation based on our model of how uncertain the sensor is, and may depend on $z$ or be constant.

In the motion prediction step, we \textbf{sampled} randomly from a Gaussian distribution to move each particle by a slightly different amount.
Here we just \textbf{read off} a value from a Gaussian function to obtain a likelihood for each particle.

\subsection{Robust Likelihood for Sonar Update}
A \textbf{robust} likelihood function models the fact that garbage values are sometimes reported by real sensors.

These functions have ``heavy tails'', achieved by simply adding a constant to the likelihood function.
This means that there is some constant probability that the sensor will return a garbage value, uniformly distributed across the range of the sensor:

\begin{figure}[h]
  \caption{A robust likelhood function}
  \includegraphics[scale=0.4]{robust}
  \centering
\end{figure}

\[
  p(z \mid m) \propto e^{\frac{-(z - m)^2}{2 \sigma^2_s}} + K
\]

The effect is that the filter is less aggressive in killing particles which are far from agreeing with measurements.
An occasional garbage measurement does not kill all particles in good positions.

\subsection{Likelihood for Sonar Update: Angle}
\begin{figure}[h]
  \caption{Robot facing a wall with angle shown.}
  \includegraphics[scale=0.4]{sonarangle}
  \centering
\end{figure}

If the angle between the sonar direction and the normal to the wall is too great, the sonar is less likely to give a sensible reading and should be ignored.
\[
  \beta = \cos^{-1} \left( \frac{\cos \theta (A_y - B_y) + \sin \theta (B_x - A_x)}{\sqrt{(A_y - B_y)^2 + (B_x - A_x)^2}} \right)
\]

\subsection{Normalisation}
\begin{itemize}
  \item The weights of all particles should be scaled so that they add up to 1.
  \item Calculate $\sum_{i = 1}^N w_i$ and divide all existing weights by this:
    \[
      w_{i(new)} = \frac{w_i}{\sum_{i = 1}{N} w_i}
    \]
\end{itemize}

\subsection{Resampling}
Generating a new set of $N$ particles which all have equal weights $\frac{1}{N}$, but whose spatial distribution now reflects the probability distribution.

We generate each of $N$ new particles by:
\begin{itemize}
  \item Copy the state of one of the previous set of particles with probability according to its weight.
  \item We do this by generating the \textbf{cumulative probability distribution} of the particles, generating a number between $0$ and $1$, then picking the particle whose cumulative probability intersects.
\end{itemize}

For efficiency, we can skip the normalisation step and resample directly from an unnormalised distribution.

\subsection{Compass Sensor}
A digital compass gives the robot the ability to estimate its rotation without drift:
\begin{itemize}
  \item A digital compass and a single sonar sensor puts us in a position close to having a full sonar ring.
  \item The robot could stop and rotate on the sport every so often to simulate the ing.
  \item The compass would be monitored as the robot continuously moves, and this is fed into the filter.
\end{itemize}

\subsection{Compass Sensor: $P(\beta, x_i)$}
The compass measures bearing $\beta$ relative to north.
The likelihood depends only on the $\theta$ part of the particle.
If the compass is working perfectly:
\[
  \beta = \gamma - \theta
\]
where $\gamma$ is the magnetic bearing of the $x$ coordinate axis frame $W$.

We should assess the uncertainty in the compass and set a likelihood which depends on the difference between $\beta$ and $\gamma - \theta$.
Particles far from the right orientation will get low weights.

\section{Place Recognition}
\subsection{Kidnapped Robot}
In MCL, global localisation can be attempted but requires many particles and may take many movements and measurements to find the right location.

We would expect better performance with more informative sensing, e.g.\ a ring of sonar sensors making measurements at the same time rather than just one.

\begin{figure}[h]
  \caption{After one measurement with only one sonar, the weights of consistent particles will increase; further measurements and movement are required to lock down position and ambiguities may still arise.}
  \includegraphics[scale=0.4]{onesonar}
  \centering
\end{figure}

\begin{figure}[h]
  \caption{With one compass and one sonar, ambiguity is reduced.}
  \includegraphics[scale=0.4]{compasssonar}
  \centering
\end{figure}

\subsection{Global Localisation via Recognition}
An alternative relocalisation technique involves making a lot of measurements at a number of chosen locations and learning their characteristics.
\begin{itemize}
  \item This requires \textbf{training}, but no prior map.
  \item The robot can only recognise learned locations.
\end{itemize}
An example of this is spinning the robot and taking a regularly spaced set of sonar measurements.

\subsection{Measuring to Learn a Location}
The robot is placed in each target location to learn its appearance.
The raw measurements are stored to describe the location, this is called a place descriptor or \textbf{signature}.

\begin{figure}[h]
  \caption{Histogram of depth measurements at a location.}
  \includegraphics[scale=0.4]{locationdepth}
  \centering
\end{figure}

\subsection{Place Recognition}
When placed in one of the locations, it must take a set of measurements and compare with saved signatures to see which best matches the measurements.

Two histograms can be compared with a correlation test, measuring the sum of the squared differences.
\[
  D_k = \sum_i (H_m(i) - H_k(i))^2
\]
where $D_k$ is the difference between the new measurement histogram $H_m(i)$ and the saved signature histogram $H_k(i)$.

The saved location with the lowest $D_k$ is the most likely candidate, but we should also check if $D_k$ is below a threshold in case it is in a new location.

\subsection{Estimating Orientation}
If the test histogram and one of the signatures can be brought into close agreement by only a shift, the robot is in the same place but rotated.

The amount of \textbf{shift} to get the best agreement is a measurement of rotation.

\subsection{Depth Measurement Histogram}
\begin{figure}[h]
  \caption{Frequency of depth measurements.}
  \includegraphics[scale=0.4]{freqdepth}
  \centering
\end{figure}
To save computation cost of trying every shift, we can build a signature which is invariant to robot rotation, such as a histogram of occurences of certain depth measurements.

Once the correct location has been found, the shifting procedure only needs to occur for that one location.

\section{Probabilistic Occupancy Grid Mapping}
In the case that a robot's localisation is known, the goal is to infer which parts of the environment around a robot are naviagable free-space, and which contain obstacles.

\begin{itemize}
  \item Rather than building a parametric grid of the positions of walls, we use a regular grid representation.
  \item Occupancy grids accumulate the uncertainty from the sensors to solidify towards precise maps.
\end{itemize}

\subsection{Occupancy Grid Map Representation}
\begin{itemize}
  \item We define an area on the ground we would like to map, and choose a square grid cell size.
  \item For each cell $i$, we store and update a \textbf{probability of occupancy} $P(O_i)$.
  \item $P(E_i)$ is the corresponding probability that the cell is empty, with $P(O_i) + P(E_i) = 1$.
  \item We initialise the occupancy probabilities for unexplored space to a constant prior value.
\end{itemize}
Occupancy maps are often visualised with a greyscale value for each cell, black for $P(O_i) = 1$ and white for $P(O_i) = 0$.

\subsection{Update after Sonar Measurement}
\begin{figure}[h]
  \caption{Sonar beam on an occupancy grid.}
  \includegraphics[scale=0.4]{sonarbeam}
  \centering
\end{figure}

For each cell we want to update the occupancy probability to take account of the new measurement $Z$.

Suppose that $Z = d$, this provides evidence that cells around distance $d$ in front of the robot are more likely to be occupied.
Cells in front of the robot $< d$ are more likely to be empty.

A sonar beam is not a perfect ray, it has a width as it spreads out so we must take account of this.

For each cell, we test if it lies within the beam.
We learn nothing about cells beyond the beam width or beyond the measured depth.

\subsection{Bayesian Update of Occupancy}
For each cell, we apply Bayes rule to get a posterior probability for each cell:
\[
  P(O_i \mid Z) = \frac{P(Z \mid O_i) P(O_i)}{P(Z)}
\]

As in MCL, we can avoid calculating $P(Z)$ by also calculating $P(E_i \mid Z)$ and normalising as we know $P(O_i \mid Z) + P(E_i \mid Z) = 1$.
\[
  P(E_i \mid Z) = \frac{P(Z \mid E_i) P(E_i)}{P(Z)}
\]

\subsection{Log Odds Representation}
If we take the ratio of the two Bayes rule expressions:
\[
  \left( \frac{P(O_i \mid Z)}{P(E_i \mid Z)}\right) = 
  \left( \frac{P(Z \mid O_i)}{P(Z \mid E_i)}\right)
  \times
  \left( \frac{O_i}{E_i}\right)
\]

We can use the odds notation: $o(A) = \frac{P(A)}{P(\bar{A})}$
\[
  o(O_i \mid Z) = 
  \left( \frac{P(Z \mid O_i)}{P(Z \mid E_i)}\right)
  \times
  o(O_i)
\]

Taking logs:
\[
  \ln o(O_i \mid Z) = 
  \ln \left( \frac{P(Z \mid O_i)}{P(Z \mid E_i)}\right)
  \times
  \ln o(O_i)
\]

In this form, we store $\ln o(O_i)$ and update it additively.
Cells with occupancy probability $0.5$ will have log odds 0; positive log odds mean probability $> 0.5$; while negative mean $< 0.5$.

We normally cap log odds at certain positive and negative limits.

\subsection{Likelihood Model of Sensor}
We need models for the likelihood function which models sensor performance.
We can consider directly the ratio of likelihoods we need to update log odds.
\begin{itemize}
  \item Log odds update $U = \ln \frac{P(Z \mid O_i)}{P(Z \mid E_i)}$: for each cell $P(Z \mid O_i)$ is the probability of obtaining the sensor value given that the cell is occupied; $P(Z \mid E_i)$ is the probability of obtaining the value given it is empty.
  \item For cells within the sonar beam but closer than $Z = d$: $\frac{P(Z \mid O_i)}{P(Z \mid E_i)} < 1$, we choose a constant negative value for $U$.
  \item For cells within the sonar at around $Z = d$: $\frac{P(Z \mid O_i)}{P(Z \mid E_i)} > 1$, we can choose a constant positive value for $U$.
\end{itemize}

Note that we are oversimplifying by assuming occupancies for each cell are independent.

\subsection{Occupancy Grid Mapping}
Over time and robot motion, measurements accumulate and the map converges to black and white, indicating definite knowledge.

An occupancy map's probability values must be thresholded if a decision is to be made about traversable areas.

Large maps are very memory intensive and can drift due to localisation uncertainty.

\section{Simultaneous Localisation and Mapping}
\textit{A body with quantitive sensors moves through a previously unknown, static environment, mapping it and calculating its egomotion.}

Why do we need SLAM?
\begin{itemize}
  \item Fully autonomous robot.
  \item Little or nothing known in advance about environment.
  \item Unable to place aritificial beacons, or use GPS.
  \item Allow robot to know where it is.
\end{itemize}

SLAM builds a map incrementally, and localises it with respect to that map as it grows and is gradually refined.

\subsection{Features for SLAM}
(Most algorithms) make maps of natural scene features.
\begin{itemize}
  \item Laser/sonar: wall segments, planes, corners, etc.\
  \item Vision: salient point features, lines, textured surfaces.
\end{itemize}

Features should be distinctive and easily recognisable from different viewpoints to enable reliable matching (\textbf{correspondence} or \textbf{data association}).

\subsection{Propagating Uncertainty}
We assume the robot is the only thing that moves - \textbf{the world is static}.

With this assumption, we can extend probabilistic estimation (from just the robot state as in MCL) to the features of the map as well.
We store and update a joint distribution over the states of both the robot and the mapped world.
When we re-measure known features, the uncertainty of the estimation of all other features also decreases.

New features are gradually discovered, so the dimension of the joint estimation problem grows.

\subsection{SLAM with Joint Gaussian Uncertainty}
The most common and efficient way to represent the high-dimensional probability distributions we need to propagate in SLAM is as a joint Gaussian distribution.
We make updates via the \textbf{Extended Kalman Filter}.

We represent the PDF with state vector and covariance matrix:
\begin{align*}
  \hat{x} &= 
  \begin{pmatrix}
    \hat{x}_v \\
    \hat{y}_1 \\
    \hat{y}_2 \\
    \vdots
  \end{pmatrix}
  & P &=
  \begin{pmatrix}
    P_{xx} & P_{xy_1} & P_{xy_2} & \dots \\
    P_{y_1x} & P_{y_1y_1} & P_{y_1y_2} & \dots \\
    P_{y_2x} & P_{y_2y_1} & P_{y_2y_2} & \dots \\
    \vdots & \vdots & \vdots & \ddots
  \end{pmatrix}
\end{align*}

The state vector contains the robot state and all the feature states.
$x_v$ is the robot state (e.g.\ $(x, y, \theta)$ in 2D); $y_i$ is a feature state (e.g.\ $(X, Y)$ in 2D).

\subsection{SLAM using Active Vision}
\begin{figure}[h]
  \caption{Active vision on a 3-wheel robot base.}
  \includegraphics[scale=0.4]{activevision}
  \centering
\end{figure}

\begin{itemize}
  \item \textbf{Stereo active vision}.
  \item Automatic fixated \textbf{active mapping and measurement} of arbitrary scene features.
  \item Sparse mapping.
\end{itemize}

\subsection{SLAM with Ring of Sonars}
\begin{figure}[h]
  \caption{Visualisation of a robot using a ring of sonars.}
  \includegraphics[scale=0.4]{ringofsonars}
  \centering
\end{figure}

\subsection{SLAM with a Single Camera}
\begin{figure}[h]
  \caption{Keypoints detected using MonoSLAM.}
  \includegraphics[scale=0.4]{monoslam}
  \centering
\end{figure}
Frontend: keypoints are detected in successive images and associated with 3D point in the world.

Backend: the pose of the camera(s) and 3D points that best explain these keypoint measurements are estimated.

\subsection{Limits of Metric SLAM}
Purely metric probabilistic SLAM is limited to small domains:
\begin{itemize}
  \item Poor computational scaling of probabilistic filters.
  \item Growth in uncertainty at large distances from map origin - inaccurate representation of uncertainty.
  \item Data association difficult at high uncertainty.
\end{itemize}

\subsection{Large Scale Localisation and Mapping}
A metric/topological approach which approximates full metric SLAM is used in practical modern solutions.
They need the following elements:
\begin{itemize}
  \item Local metric mapping to estimate trajectory and make local maps.
  \item Place recognition to perform \textbf{loop closure} or relocalise the robot when lost.
  \item Map optimisation/relaxation to optimise a map when loops are closed.
\end{itemize}

One very effective way to detect loop closures is to save images at regular intervals and use an image retrieval approach (where each image is represented using a Visual Bag of Words).

\subsection{Pure Topological SLAM}
\begin{figure}[h]
  \caption{Topological graph.}
  \includegraphics[scale=0.4]{toplogical}
  \centering
\end{figure}

We can make a SLAM system using \textbf{only} place recognition, this is topological SLAM with a graph-based representation.

We keep a record of places we have visited and how they connect together, without any explicit geometry information.

\subsection{Adding Metric Information to Graph Edges}
\begin{figure}[h]
  \caption{Applying relaxation when loop closure detected.}
  \includegraphics[scale=0.4]{relaxation}
  \centering
\end{figure}

\begin{itemize}
  \item Edges between linked nodes are annotated with relative motion information - from local mapping or incemental information from odometry.
  \item Apply the \textbf{pose graph optimisation (relaxation)} algorithm - compute the set of node positions maximally probable given both the metric and topological constraints.
    (This only has an effect when there are loops).
\end{itemize}

\subsection{Simple Large Scale SLAM: RATSLAM}
\begin{itemize}
  \item Very simple visual odometry gives rough trajectory.
  \item Simple visual place recognition - many loop closures.
  \item Map relaxation/optimisation to build global map.
\end{itemize}

\subsection{More Accurate Large-Scale Monocular SLAM: ORB-SLAM}
Tracking and mapping recognisable features:
\begin{itemize}
  \item Very accurate visual odometry trajectory.
  \item Visual place recognition based on 2D image features with binary descriptors for very fast matching.
  \item Pose graph/map optimisation for global consistency.
\end{itemize}

\subsection{ElasticFusion}
\begin{itemize}
  \item Reliable room-scale dense mapping.
  \item Maps a scene with millions of surfels and corrects loop closures.
  \item Relies on depth camera and GPU processing.
\end{itemize}

\section{Planning}
If a robot has a map of the environment, how can it plan to get from one place to another?

How can it plan a path around a complicated set of obstacles?

\subsection{Local Planning}
In the \textbf{Dynamic Window Approach}:
\begin{enumerate}
  \item Consider robot dynamics and possible changes in motion it can make within small time $dt$.
  \item For each possible motion look ahead a longer time $\tau$, calculate the benefit/cost based on distance from target and obstacles.
  \item Choose best and execute for $dt$.
  \item Repeat.
\end{enumerate}

\subsection{Dynamic Window Approach for Differential Drive Robot}
\begin{itemize}
  \item The robot can control $v_L, v_R$ up to maximum values.
  \item The max acceleration $\times dt$ is the maximum change we can make to $v_L$ or $v_R$ in time $dt$.
  \item There are nine possible actions in each step, either of $v_L$ or $v_R$ can go up, go down, or stay the same.
  \item After longer time $\tau$, we know in the general case the robot moves on a circular arc:
    \[
      \begin{pmatrix}
        x_{new} \\
        y_{new} \\
        \theta_{new}
      \end{pmatrix}
      =
      \begin{pmatrix}
        x + R(\sin(\Delta\theta + \theta) - \sin\theta) \\
        y - R(\cos(\Delta\theta + \theta) - \cos\theta) \\
        \theta + \Delta\theta
      \end{pmatrix}
    \]
  \item For each motion calculate the benefit and cost.
    Benefit is one weight times the amount the robot would be moved closer to its target at $(T_x, T_y)$:
    \[
      D_F = \sqrt{(T_x - x)^2 + (T_y - y)^2} - \sqrt{(T_x - x_{new})^2 + (T_y - y_{new})^2} 
    \]
    \[
      B = W_B \times D_F
    \]
  \item Subtract from this a cost based on the distance the robot would be from the closest obstacle $(O_x, O_y)$:
    \[
      C = W_C \times (D_{safe} - (\sqrt{(O_x - x_{new})^2 + (O_y - y_{new})^2A} - r_{robot} - r_{obstacle}))
    \]
    We find the closest by searching through obstacles; $D_{safe}$ is some distance we would ideally like to stay from contact; $r_{robot}$ and $r_{obstacle}$ are the robot and obstacle radii.
  \item Choose the path with maximum $B - C$, execute for $dt$.
\end{itemize}

\subsection{Global Planning: Wavefront Method}
\begin{figure}[h]
  \caption{Wavefront visualisation.}
  \includegraphics[scale=0.4]{wavefront}
  \centering
\end{figure}

\begin{itemize}
  \item Brute force \textbf{flood fill} breadth first search of whole environment.
  \item Slow, but guaranteed to find shortest route.
\end{itemize}

\subsection{Global Planning: Rapidly Exploring Randomised Trees (RRT) Method}
\begin{figure}[h]
  \caption{RRT visualisation.}
  \includegraphics[scale=0.4]{rrt}
  \centering
\end{figure}

\begin{itemize}
  \item Algorithm grows a tree of connected nodes by randomly sampling points and extending the tree a short step from the closest node.
  \item Expands rapidly into new areas, but without the same guarantees.
\end{itemize}

\end{document}

