\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{graphicx}
\usepackage{gensymb}

\graphicspath{ {./imgs/} }

\setlength{\parindent}{0pt}

\title{Graphics (CO317)}
\author{Michael Tsang}

\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{theo}{Theorem}
\newtheorem{lem}{Lemma}

\begin{document}

\maketitle

\section{Projections and Transformations}
\subsection{Device Independence}
\textbf{Device independence} allows us to resize or transport a picture to a different operating system, and have it fit exactly in the window where we place it.
We define a \textbf{world coordinate system} when drawing objects, based on the window size.

In order to implement a world coordinate system, we need to be able to translate between world coordinates and the device or pixel coordinates - this is done by getting the pixel coordinates of the window.

\begin{figure}[htb!]
  \caption{World coordinate window against viewport on the screen.}
  \includegraphics[scale=0.2]{normalisation}
  \centering
\end{figure}

We then perform normalisation to compute the pixel coordinates from the world coordinates, using simple ratios.
For the $x$ direction:
\[
  X_v = \frac{(X_w - W_{xmin})(V_{xmax} - V_{xmin})}{W_{xmax} - W_{xmin}} + V_{xmin}
\]
with a similar expression for $Y_v$.

From the known values, we can form a simple pair of linear equations:
\begin{align*}
  X_v &= AX_w + B \\
  Y_v &= CY_w + D
\end{align*}

If the window is moved or resized, we must re-calculate the constants $A, B, C, D$.

\subsection{Representing Planar Polygons}
Most graphical scenes and objects are built out of planar polyhedra, three-dimensional objects whose faces are all planar polygons (\textbf{faces} or \textbf{facets}).
To describe objects made of polygons, we need the \textbf{locations} of the vertices and their \textbf{topology}:
\begin{itemize}
  \item Numerical Data - 3D coordinates of vertices.
  \item Topological Data - what is connected to what.
\end{itemize}

\subsection{Projections of Wire-Frame Models}
To draw a 3D wire frame model, we must first convert the points to 2D representation - \textbf{projection}.
A \textbf{projector} takes a point on the object to a point on the 2D projection surface.

\subsection{Orthographic Projections}
\begin{figure}[htb!]
  \caption{Orthographic projection.}
  \includegraphics[scale=0.3]{parallel}
  \centering
\end{figure}
Assumptions:
\begin{itemize}
  \item Viewpoint at $z = - \infty$.
  \item Plane of projection is $z = 0$.
\end{itemize}

All projectors have the same direction:
\[ \bm{d} = \begin{pmatrix} 0 \\ 0 \\ - 1 \end{pmatrix} \]

Each projection line has the equation:
\[ \bm{P} = \bm{V} + \mu\bm{d} \]

We simply take the $x$ and $y$ coordinates:
\[ P = \begin{pmatrix} V_x \\ V_y \\ 0 \end{pmatrix} \]
since the projection plane is $z = 0$.

\subsection{Perspective Projections}
Orthographic projections are fine in cases where we do not care about depth.
In \textbf{perspective projection}, all the projectors pass through one point in space, the \textit{centre of projection}.
If the centre of projection is on the opposite side of the plane of projection compared to the 3D object, the orientation of the image is the same as the object.
Otherwise if the centre is between the plane and the object, the image is inverted.

\begin{figure}[htb!]
  \caption{Perspective projection where the viewpoint is on the opposite side of the object.}
  \includegraphics[scale=0.3]{perspective}
  \centering
\end{figure}

Assumptions:
\begin{itemize}
  \item Centre of projection is at the origin.
  \item Projection plane is at a constant $z$ value, $z = f$.
\end{itemize}

The perspective projector equation from vertex $\bm{V}$ is:
\[ \bm{P} = \mu \bm{V} \]

This means:
\begin{align*}
  P_x = \mu_p V_x = \frac{fV_x}{V_z} && P_y = \mu_p V_y = \frac{fV_y}{V_z}
\end{align*}
since $P_z = f$.

\subsection{Space Transformations}
We want to transform for:
\begin{itemize}
  \item Changing viewpoint.
  \item Animating objects.
  \item Multiple instances.
  \item Reflections and other special effects.
\end{itemize}

Transformation matrices can be used for nearly all simple transformations, except for translation using normal Cartesian coordinates.
We thus introduce \textbf{homogeneous coordinates}.

\subsubsection{Homogeneous Coordinates}
To express a point in homogeneous coordinates, we introduce a fourth ordinate:
\[ P = \begin{pmatrix} p_x \\ p_y \\ p_z \\ s \end{pmatrix} \]

The fourth ordinate is a \textbf{scale factor}:
\[ (p_x, p_y, p_z, s) \Leftrightarrow (\frac{p_x}{s}, \frac{p_y}{s}, \frac{p_z}{s}) \]
In most cases $s = 1$.

\subsubsection{Translation}
To represent translation:
\[
  \begin{pmatrix}
    1 & 0 & 0 & t_x \\
    0 & 1 & 0 & t_y \\
    0 & 0 & 1 & t_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    p_x \\
    p_y \\
    p_z \\
    1
  \end{pmatrix}
  =
  \begin{pmatrix}
    p_x + t_x \\
    p_y + t_y \\
    p_z + t_z \\
    1
  \end{pmatrix}
\]

The inverse is:
\[
  \begin{pmatrix}
    1 & 0 & 0 & -t_x \\
    0 & 1 & 0 & -t_y \\
    0 & 0 & 1 & -t_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\subsubsection{Scaling}
To represent scaling:
\[
  \begin{pmatrix}
    s_x & 0 & 0 & 0 \\
    0 & s_y & 0 & 0 \\
    0 & 0 & s_z & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix}
    p_x \\
    p_y \\
    p_z \\
    1
  \end{pmatrix}
  =
  \begin{pmatrix}
    s_xp_x \\
    s_yp_y \\
    s_zp_z \\
    1
  \end{pmatrix}
\]

The inverse is:
\[
  \begin{pmatrix}
    1/s_x & 0 & 0 & 0 \\
    0 & 1/s_y & 0 & 0 \\
    0 & 0 & 1/s_z & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\subsubsection{Combining Transformations}
To combine transformations, we can multiply out their matrices and apply the transformation with the resultant matrix.

\begin{figure}[htb!]
  \caption{Importance of the order of transformations, $\bm{T} \cdot \bm{S} \neq \bm{S} \cdot \bm{T}$.}
  \includegraphics[scale=0.2]{combining}
  \centering
\end{figure}

\subsubsection{Rotation}
To define rotation we need an axis and an angle.
Matrices for rotation around the three Cartesian axes are:
\begin{align*}
  \mathcal{R}_x &=
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & \cos \theta & -\sin \theta & 0 \\
    0 & \sin \theta & \cos \theta & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix} \\
  \mathcal{R}_y &=
  \begin{pmatrix}
    \cos \theta & 0 & \sin \theta & 0 \\
    0 & 1 & 0 & 0 \\
    -\sin \theta & 0 & \cos \theta & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix} \\
  \mathcal{R}_z &=
  \begin{pmatrix}
    \cos \theta &  -\sin \theta & 0 & 0 \\
    \sin \theta & \cos \theta & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\end{align*}

\begin{figure}[htb!]
  \caption{Rotation of a coordinate ($z$-axis goes into the page).}
  \includegraphics[scale=0.3]{deriverotate}
  \centering
\end{figure}

This assumes a left had axis system:
\begin{itemize}
  \item Rotation is \textbf{anti-clockwise} when looking along the axis of rotation.
  \item Rotation is \textbf{clockwise} when looking back towards the origin from the positive side of the axis.
\end{itemize}

To invert rotation, we rotate through an angle of $- \theta$, and note the follow relations:
\begin{align*}
  \cos(-\theta) &= \cos(\theta) & \sin(-\theta)=-\sin(\theta)
\end{align*}

Then for example:
\[
  \mathcal{R}_z(-\theta) = 
  \begin{pmatrix}
    \cos \theta & \sin \theta & 0 & 0 \\
    - \sin \theta & \cos \theta & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\section{Transformations for Animation}
In a viewer-centred application, we wish to view the scene from a moving position.
When the viewpoint changes, we need to transform all the coordinates of the scene.

\subsection{Flying Sequences}
Each moved viewpoint is a change of origin:
\begin{itemize}
  \item Let the required viewpoint be $\bm{C} = (C_x, C_y, C_z)$.
  \item Let the required direction be $\bm{d} = \begin{pmatrix} d_x \\ d_y \\ d_z \end{pmatrix}$.
\end{itemize}

The required transformation is split into three parts:
\begin{enumerate}
  \item Translation of the origin.
  \item Rotation about the $y$-axis.
  \item Rotation about the $x$-axis.
\end{enumerate}

\subsubsection{Translation of the Origin}

\begin{figure}[htb!]
  \caption{Translation of the origin.}
  \includegraphics[scale=0.2]{transorigin}
  \centering
\end{figure}

We apply the transformation matrix:
\[
  \mathcal{A} =
  \begin{pmatrix}
    1 & 0 & 0 & -C_x \\
    0 & 1 & 0 & -C_y \\
    0 & 0 & 1 & -C_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\subsubsection{Rotation about the $y$-axis}
\begin{figure}[htb!]
  \caption{Rotation about the $y$-axis.}
  \includegraphics[scale=0.2]{roty}
  \centering
\end{figure}

We rotate about $y$ until $d$ is in the $y-z$ plane ($x = 0$):
\begin{align*}
  \lVert \bm{v} \lVert = v = \sqrt{d_x^2 + d_z^2} && \cos \theta = d_z / v && \sin \theta = d_x / v
\end{align*}

\[
  \mathcal{B} =
  \begin{pmatrix}
    \cos \theta & 0 & -\sin \theta & 0 \\
    0 & 1 & 0 & 0 \\
    \sin \theta & 0 & \cos \theta & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  =
  \begin{pmatrix}
    d_z / v & 0 & -d_x / v & 0 \\
    0 & 1 & 0 & 0 \\
    d_x / v & 0 & d_z / v & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\subsubsection{Rotation about the $x$-axis}
\begin{figure}[htb!]
  \caption{Rotation about the $x$-axis.}
  \includegraphics[scale=0.2]{rotx}
  \centering
\end{figure}

We then rotate about $x$ until $d$ points along the $z$-axis:
\begin{align*}
  v = \sqrt{d_x^2 + d_z^2} && \cos \phi = v / \lvert d \rvert && \sin \phi = d_y / \lvert d \rvert
\end{align*}

\[
  \mathcal{C} =
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & \cos \phi & -\sin \phi & 0 \\
    0 & \sin \phi & \cos \phi & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  =
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & v / \lvert d \rvert & -d_y / \lvert d \rvert & 0 \\
    0 & d_y / \lvert d \rvert & v / \lvert d \rvert & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\subsubsection{Combining the Matrices}
\[
  \mathcal{T} = \mathcal{CBA}  
\]
Then for every point $\bm{P}$ in the scene, we calculate:
\[
  \bm{P}_t = \mathcal{T}\bm{P}  
\]
The view is now in canonical form, and we can apply the standard perspective or orthographic projection.

\subsubsection{Verticals}
\begin{figure}[htb!]
  \caption{Order of rotation affecting inversion.}
  \includegraphics[scale=0.2]{invert}
  \centering
\end{figure}

Usually the $y$ direction is treated as vertical, by doing the $R_y$ transformation first, things will work out correctly.

However if we do the $R_x$ first, we could end up inverting the vertical.

\subsection{Rotation about a General Line}
To perform this transformation, we:
\begin{enumerate}
  \item Make the line of rotation one of the Cartesian axes - $\mathcal{CBA}$.
  \item Rotate about the chosen axis - the $z$-axis is aligned with the general line.
  \item \textbf{Restore} the line to its original place - invert the initial matrices.
\end{enumerate}

\[
  \mathcal{T} = \mathcal{A}^{-1} \mathcal{B}^{-1} \mathcal{C}^{-1} \mathcal{R}_z \mathcal{CBA}  
\]

\subsubsection{Other Effects}
Similar effects can be created using this approach.
\begin{eg}
  To make an object shrink (and stay in place):
  \begin{enumerate}
    \item Move the object to the origin.
    \item Apply a scaling matrix.
    \item Restore (move) the object to its place.
  \end{enumerate}
\end{eg}

\subsection{Projection by Matrix Multiplication}
\subsection{Orthographic Projection Matrix}
\[
  \mathcal{M}_o =
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\subsection{Perspective Projection Matrix}
\[
  \mathcal{M}_p =
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 0 & 1 / f & 0
  \end{pmatrix}
\]

We must then normalise the result to get:
\[
  \begin{pmatrix} xf/z \\ yf/z \\ f \\ 1 \end{pmatrix}  
\]
as required.

\subsection{Singularity}
Both projection matrices are \textbf{singular}, notice each has a column of zeroes - they \textbf{cannot be inverted}.

\subsection{Homogeneous Coordinates and Vectors}
\begin{itemize}
  \item \textbf{Position vectors} - $\begin{pmatrix} x \\ y \\ z \\ s \end{pmatrix}$.
    \begin{itemize}
      \item $s > 0$.
      \item Can be normalised into Cartesian form.
      \item \textit{Fixed point in space.}
    \end{itemize}
  \item \textbf{Direction vectors} - $\begin{pmatrix} x \\ y \\ z \\ 0 \end{pmatrix}$.
    \begin{itemize}
      \item $s = 0$.
      \item Have direction and magnitude.
      \item \textit{Not associated with a particular point.}
    \end{itemize}
\end{itemize}

\subsubsection{Addition of Vectors}
Adding \textbf{two direction vectors} results in a direction vector (notice the zero final ordinate):
\[
  \begin{pmatrix} x_i \\ y_i \\ z_i \\ 0 \end{pmatrix} 
  +
  \begin{pmatrix} x_j \\ y_j \\ z_j \\ 0 \end{pmatrix} 
  =
  \begin{pmatrix} x_i + x_j \\ y_i + y_j \\ z_i + z_j \\ 0 \end{pmatrix} 
\]

Adding a \textbf{direction vector to a position vector} results in a position vector:
\[
  \begin{pmatrix} X \\ Y \\ Z \\ 1 \end{pmatrix} 
  +
  \begin{pmatrix} x \\ y \\ z \\ 0 \end{pmatrix} 
  =
  \begin{pmatrix} X + x \\ Y + y \\ Z + z \\ 1 \end{pmatrix} 
\]

Adding \textbf{two position vectors} results in their mid-point:
\[
  \begin{pmatrix} X_a \\ Y_a \\ Z_a \\ 1 \end{pmatrix} 
  +
  \begin{pmatrix} X_b \\ Y_b \\ Z_b \\ 1 \end{pmatrix} 
  =
  \begin{pmatrix} X_a + X_b \\ Y_a + Y_b \\ Z_a + Z_b \\ 2 \end{pmatrix} 
  =
  \begin{pmatrix} (X_a + X_b) / 2 \\ (Y_a + Y_b) / 2 \\ (Z_a + Z_b) / 2 \\ 1 \end{pmatrix} 
\]

\subsection{Transformation Matrices}
\subsubsection{Structure}
\[
  \begin{pmatrix}
    q_x & r_x & s_x & C_x \\
    q_y & r_y & s_y & C_y \\
    q_z & r_z & s_z & C_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
\]
\begin{itemize}
  \item The bottom row is always $\begin{matrix}0&0&0&1\end{matrix}$.
  \item The columns comprise of three direction vectors and one position vector.
\end{itemize}

\subsubsection{Characteristics}
When we multiply a direction vector, the last ordinate ensures it is not affected by translation:
\[
  \begin{pmatrix}
    q_x & r_x & s_x & C_x \\
    q_y & r_y & s_y & C_y \\
    q_z & r_z & s_z & C_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix} * \\ * \\ * \\ 0 \end{pmatrix}
  =
  \begin{pmatrix} * \\ * \\ * \\ 0 \end{pmatrix}
\]

When we multiply a position vector, the last ordinate ensures all vectors have the same displacement:
\[
  \begin{pmatrix}
    q_x & r_x & s_x & C_x \\
    q_y & r_y & s_y & C_y \\
    q_z & r_z & s_z & C_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix} * \\ * \\ * \\ 1 \end{pmatrix}
  =
  \begin{pmatrix} * + C_x \\ * + C_y \\ * + C_z \\ 1 \end{pmatrix}
\]

If we do not shear the object, $q$, $r$, $s$ will remain orthogonal:
\[
  q \cdot r = r \cdot s = q \cdot s = 0  
\]

\subsubsection{Meaning of the Individual Columns}
When we multiply the matrix by unit vectors, we get the individual columns of the matrix:
\[
  \begin{pmatrix}
    q_x & r_x & s_x & C_x \\
    q_y & r_y & s_y & C_y \\
    q_z & r_z & s_z & C_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix} 1 & 0 & 0 & 0 \end{pmatrix}
  =
  \begin{pmatrix} q_x & q_y & q_z & 0 \end{pmatrix}
\]

This means that the columns are the original axis system after transforming to the new coordinate system:
\begin{itemize}
  \item $\bm{q}$ is the transformed $x$-axis.
  \item $\bm{r}$ is the transformed $y$-axis.
  \item $\bm{s}$ is the transformed $z$-axis.
  \item $\bm{C}$ is the transformed origin.
\end{itemize}

\subsubsection{Effect of a Transformation Matrix}
\begin{figure}[htb!]
  \caption{Effect of a transformation matrix.}
  \includegraphics[scale=0.2]{effecttransform}
  \centering
\end{figure}

We get the old axes and origin in the new coordinate system:
\[
  \begin{pmatrix}
    q_x & r_x & s_x & C_x \\
    q_y & r_y & s_y & C_y \\
    q_z & r_z & s_z & C_z \\
    0 & 0 & 0 & 1
  \end{pmatrix}
  =
  \begin{bmatrix} \bm{q} & \bm{r} & \bm{s} & \bm{C} \end{bmatrix}
\]

\subsection{General Viewing Matrix Transformation}
Normally, we are not given the transformation matrix, but the view direction $\bm{d}$ and location $\bm{C}$ instead.

\subsubsection{Dot Product}
\begin{defn}
  The \textbf{dot product} is defined:
  \[
    \bm{P} \cdot \bm{u} = \lvert \bm{P} \rvert \lvert \bm{u} \rvert \cos \theta  
  \]
\end{defn}

\begin{figure}[htb!]
  \caption{The dot product as an ordinate of $\bm{P}$.}
  \includegraphics[scale=0.2]{dotproduct}
  \centering
\end{figure}

\begin{itemize}
  \item If $\bm{u}$ is a unit vector, then $\bm{P} \cdot \bm{u} = \lvert \bm{P} \rvert \cos \theta$.
  \item If $\bm{u}$ is along a coordinate axis, then $\bm{P} \cdot \bm{u}$ is the ordinate of $\bm{P}$ in the direction of $\bm{u}$.
\end{itemize}

\subsubsection{Changing Axes by Projection}
\begin{figure}[htb!]
  \caption{Transforming a point $\bm{P}$ to a different axes.}
  \includegraphics[scale=0.2]{changingaxes}
  \centering
\end{figure}

We can express projections using the dot product:
\begin{alignat*}{2}
  P_x^t &= (\bm{P} - \bm{C}) \cdot \bm{u} &&= \bm{P} \cdot \bm{u} - \bm{C} \cdot \bm{u} \\
  P_y^t &= (\bm{P} - \bm{C}) \cdot \bm{v} &&= \bm{P} \cdot \bm{v} - \bm{C} \cdot \bm{v} \\
  P_z^t &= (\bm{P} - \bm{C}) \cdot \bm{w} &&= \bm{P} \cdot \bm{w} - \bm{C} \cdot \bm{w}
\end{alignat*}

In matrix notation:
\[
  \begin{pmatrix} P_x^t \\ P_y^t \\ P_z^t \\ 1 \end{pmatrix}
  =
  \begin{pmatrix}
    u_x & u_y & u_z & -\bm{C} \cdot \bm{u} \\ 
    v_x & v_y & v_z & -\bm{C} \cdot \bm{v} \\ 
    w_x & w_y & w_z & -\bm{C} \cdot \bm{w} \\ 
    0 & 0 & 0 & 1
  \end{pmatrix}
  \begin{pmatrix} P_x \\ P_y \\ P_z \\ 1 \end{pmatrix}
\]

By maintaining values of $\bm{C}, \bm{u}, \bm{v}, \bm{w}$ throughout an animation sequence, we can write down the correct scene transformation matrix.

\subsubsection{Returning to Flying Sequences}
As we know $\bm{d}$ is the direction of the new axis, then
\[
  \bm{w} = \frac{\bm{d}}{\lvert \bm{d} \rvert}
\]

We can write $\bm{u}$ in terms of some vector $\bm{p}$ in the horizontal direction:
\[
  \bm{u} = \frac{\bm{p}}{\lvert \bm{p} \rvert}
\]
and set $p_y = 0$ to ensure it has no vertical component.

We can write $\bm{v}$ in terms of some vector $\bm{q}$ in the vertical direction:
\[
  \bm{v} = \frac{\bm{q}}{\lvert \bm{q} \rvert}
\]
and force $q_y = 1$ to ensure it has a positive $y$ component.

We now have:
\begin{align*}
  \bm{p} &= \begin{pmatrix} p_x \\ 0 \\ p_z \end{pmatrix}
  & \bm{q} &= \begin{pmatrix} q_x \\ 1 \\ q_z \end{pmatrix}
\end{align*}

It must be that:
\[
  \bm{d} = \bm{p} \times \bm{q} 
\]
since $\bm{d}$ is orthogonal to both, and the magnitude of the new vectors has not been set.

Evaluating the cross products results in:
\begin{align*}
  d_x &= -p_z \\
  d_y &= p_zq_x - p_xq_z \\
  d_z &= p_x
\end{align*}

Which means:
\[
  \bm{p} = \begin{pmatrix} d_z \\ 0 \\ -d_x \end{pmatrix} 
\]

Using the dot product with $\bm{p}$ and $\bm{q}$, then:
\[
  \bm{p} \cdot \bm{q} = p_xq_x + p_zq_z = 0
\]
which we solve with the result from the cross product:
\[
  d_y = p_zq_x - p_xq_z
\]

Once we have expressions for $\bm{p}$ and $\bm{q}$ in terms of given vector $\bm{d}$, then we have obtained:
\begin{align*}
  \bm{u} &= \frac{\bm{p}}{\lvert \bm{p} \rvert} &
  \bm{v} &= \frac{\bm{q}}{\lvert \bm{q} \rvert} &
  \bm{w} &= \frac{\bm{d}}{\lvert \bm{d} \rvert}
\end{align*}
which we use to write down:
\[
  \begin{pmatrix}
    u_x & u_y & u_z & -\bm{C} \cdot \bm{u} \\ 
    v_x & v_y & v_z & -\bm{C} \cdot \bm{v} \\ 
    w_x & w_y & w_z & -\bm{C} \cdot \bm{w} \\ 
    0 & 0 & 0 & 1
  \end{pmatrix}
\]

\section{Clipping}
\begin{defn}
  \textbf{Clipping} is the process of eliminating portions of objects outside the viewing frustum to avoid degeneracy and improve efficiency.
\end{defn}

\begin{figure}[htb!]
  \caption{The view frustum.}
  \includegraphics[scale=0.3]{frustum}
  \centering
\end{figure}

The view frustum is the boundaries of the image plane projected in 3D, consisting of a near and far clipping plane, with additional user defined clipping planes.

When do we clip?
\begin{itemize}
  \item Before perspective transform in 3D space (natural).
  \item In homogeneous coordinates after perspective transform (simple).
  \item In the transformed 3D screen space after perspective division (messy).
\end{itemize}

\subsection{Halfspace}
\begin{figure}[htb!]
  \caption{The concept of a halfspace.}
  \includegraphics[scale=0.3]{halfspace}
  \centering
\end{figure}
We can use halfspaces to perform a test on all the points in the scene, to determine if they should be discarded or drawn.

In planes, we use the general equations:
\[
  f(x, y, z) = 0 \text{ or } Ax + By + Cz + D = 0
\]

\subsection{Homogeneous Coordinates}
We can link the plane equation to homogeneous coordinates:
\[
  \bm{H} = \begin{pmatrix} A \\ B \\ C \\ D \end{pmatrix}  
\]
However, each point has an infinite number of equivalent homogeneous coordinates as they can be scaled, $\begin{pmatrix} sx & sy & sz & sw \end{pmatrix}$.

We solve this by scaling $\bm{H}$ so that $\begin{pmatrix} A & B & C \end{pmatrix}$ becomes normalized:
\[
  A^2 + B^2 + C^2 = 1  
\]

\begin{figure}[htb!]
  \caption{Using $\bm{H}$ to find the distance to a point $\bm{p}$.}
  \includegraphics[scale=0.3]{signeddistance}
  \centering
\end{figure}

Then the distance is calculated:
\[
  d = \bm{H} \cdot \bm{p} = \bm{H}^\intercal \bm{p} 
\]
since $\bm{H}$ is the normal of the plane.

$d$ is a \textbf{signed distance}:
\begin{itemize}
  \item Positive $\rightarrow$ inside the plane, pass through.
  \item Negative $\rightarrow$ outside the plane, clip (or cull or reject).
\end{itemize}

We do not actually need to normalize as we test only the sign of $\bm{H} \cdot \bm{p}$.

\subsection{Clipping with respect to View Frustum}
We test a point $\bm{p}$ against each of the 6 planes, where each normal $\bm{H}$ is oriented towards the interior.

\begin{align*}
  \bm{H}_{near} &= \begin{pmatrix} 0 & 0 & 1 & -near \end{pmatrix}^\intercal \\
  \bm{H}_{far} &= \begin{pmatrix} 0 & 0 & -1 & far \end{pmatrix}^\intercal \\
  \bm{H}_{bottom} &= \begin{pmatrix} 0 &  near & -bottom & 0 \end{pmatrix}^\intercal \\
  \bm{H}_{top} &= \begin{pmatrix} 0 & -near & top & 0 \end{pmatrix}^\intercal \\
  \bm{H}_{left} &= \begin{pmatrix} -near  & 0 & left & 0 \end{pmatrix}^\intercal \\
  \bm{H}_{right} &= \begin{pmatrix} near & 0 & -right & 0 \end{pmatrix}^\intercal
\end{align*}

We can derive the first two from observing the frustum and recalling that that the viewpoint looks along the $z$-axis.

The rest can be derived by taking points from the frustum corners:
\[
  \begin{pmatrix} l \\ b \\ n \end{pmatrix}
  \times
  \begin{pmatrix} b \\ b \\ n \end{pmatrix}
  =
  \begin{vmatrix}
    \hat{i} & \hat{j} & \hat{k} \\
    l & b & n \\
    r & b & n
  \end{vmatrix}
  =
  \begin{pmatrix} 0 & n & -b \end{pmatrix}
\]

\begin{alignat*}{2}
  \quad & (\bm{P} - \bm{P}_1) \cdot \bm{n} &&= 0 \\
  \implies & ny - bz &&= 0 \\
  \implies & \bm{H}_{bottom} &&= \begin{pmatrix} 0 & n & -b & 0 \end{pmatrix}^\intercal
\end{alignat*}

\subsection{Line-Plane Intersection}
Explicit (parametric) line equation:
\[
  \bm{L}(\mu) = \bm{p}_0 + \mu(\bm{p}_1 - \bm{p}_0) = \mu \bm{p}_1 + (1 - \mu) \bm{p}_0
\]

To compute the intersection point, we use the fact that $(\bm{P} - \bm{P}_1) \cdot \bm{n} = 0$:
\begin{enumerate}
  \item Let the intersection point be $\mu \bm{p}_1 + (1 - \mu) \bm{p}_0$.
  \item Choose $\bm{v}$ to be any point on the plane.
  \item A vector in the plane is given by $\mu \bm{p}_1 + (1 - \mu) \bm{p}_0 - \bm{v}$.
  \item Then $\bm{n} \cdot (\mu \bm{p}_1 + (1 - \mu) \bm{p}_0 - \bm{v}) = 0$
  \item Solve for $\mu$ to find the point of intersection.
\end{enumerate}

\subsubsection{Segment Clipping}
\begin{itemize}
  \item If $\bm{H} \cdot \bm{p} > 0$ and $\bm{H} \cdot \bm{q} < 0$: clip $\bm{q}$ to plane.
  \item If $\bm{H} \cdot \bm{p} < 0$ and $\bm{H} \cdot \bm{q} > 0$: clip $\bm{p}$ to plane.
  \item If $\bm{H} \cdot \bm{p} > 0$ and $\bm{H} \cdot \bm{q} > 0$: pass through.
  \item If $\bm{H} \cdot \bm{p} < 0$ and $\bm{H} \cdot \bm{q} < 0$: clipped out.
\end{itemize}

\subsubsection{Clipping against the Frustum}
For each frustum plane $\bm{H}$, we apply segment clipping.
This results in a single segment.

\subsection{Clipping and Containment}
Clipping can be carried out against any object.
We to develop a test for containment, to determine if a point is inside or outside the object, namely for convex and concave objects.

\subsubsection{Convex Objects}
Convex objects are defined by:
\begin{enumerate}
  \item A line joining any two points on the boundary lies inside the object.
  \item The object is the intersection of planar halfspaces.
\end{enumerate}
All points of the object must lie entirely to one side of each face - we use this to check if a shape is convex, and to check if a point is contained within the object.

\begin{figure}[htb!]
  \caption{Vector test for containment on a convex shape.}
  \includegraphics[scale=0.5]{containmentvector}
  \label{fig:vectortest}
  \centering
\end{figure}

The vector formulation (figure \ref{fig:vectortest}) does not require finding the plane equation of a face, but does require finding the normal vector to the plane.
This involves finding the cross product of two vectors on the plane, say two edge vectors.

\begin{figure}[htb!]
  \caption{Finding a normal vector using two edge vectors.}
  \includegraphics[scale=0.5]{edgenormal}
  \centering
\end{figure}

Care must be taken to ensure the normal found is an \textbf{inner normal}, see figure \ref{fig:innernormalcheck}.

\begin{figure}[htb!]
  \caption{Finding a normal vector using two edge vectors.}
  \includegraphics[scale=0.4]{innernormal}
  \label{fig:innernormalcheck}
  \centering
\end{figure}

\section{Graphics Pipeline}

\begin{figure}[htb!]
  \caption{The graphics pipeline.}
  \centering
  \includegraphics[scale=0.3]{pipeline}
\end{figure}

\begin{itemize}
  \item \textbf{Modelling Transformations}:
    \begin{itemize}
      \item 3D models are defined in their own coordinate system.
      \item These models are oriented within the common (world) coordinate frame.
    \end{itemize}
  \item \textbf{Illumination (Shading)}:
    \begin{itemize}
      \item Vertices are lit according to material properties, surface properties, and light sources.
      \item Local lighting model used.
    \end{itemize}
  \item \textbf{Viewing Transformation (Perspective/Orthographic)}:
    \begin{itemize}
      \item World space is mapped to camera space (matrix evaluation).
      \item Viewing position is transformed to origin and viewing direction oriented along some axis (usually $z$).
    \end{itemize}
  \item \textbf{Clipping}:
    \begin{itemize}
      \item Portions of the scene outside the view frustum are removed.
      \item Transform to \textit{Normalized Device Coordinates}.
    \end{itemize}
  \item \textbf{Projection (to Screen Space)}:
    \begin{itemize}
      \item Objects projected to the 2D imaging plane.
    \end{itemize}
  \item \textbf{Rasterizaton}
    \begin{itemize}
      \item Objects rasterized to pixels.
      \item Interpolate values inside objects.
    \end{itemize}
  \item \textbf{Visibility/Display}:
    \begin{itemize}
      \item Occlusions and transparency blending.
      \item Determines which objects closest, and therefore visible.
      \item Depth buffer.
    \end{itemize}
\end{itemize}

Real-time CGI is very computationally demanding.
Therefore we use specialized hardware - \textit{graphics processing units} (GPUs).
Most real-time graphics is based on rasterization of graphic primitives and implemented in hardware, controlled through an API such as \textit{OpenGL}.

\begin{defn}
  A \textbf{vertex} is a point in space defining geometry.
\end{defn}

\begin{defn}
  A \textbf{fragment} is a sample produced during rasterization; multiple fragments are merged into pixels.
\end{defn}

The pipeline is split into three main stages: application, geometry, and rasterization.

\subsection{Application Stage}
This is executed in software, so it cannot be divided into individual steps that are executed in a pipeline.

Changes are made to the scene as required, perhaps due to user interaction or in an animation.
The new scene with all its primitives is then forwarded to the next step of the pipeline.

The most important task performed is data management.
This includes acceleration techniques using spatial subdivision schemes that optimize the data currently stored in memory.
A modern day computer game's world and textures are much bigger than what could be loaded into the available RAM or graphics memory.

\subsection{Geometry Stage}
\begin{figure}[htb!]
  \centering
  \caption{The geometry stage.}
  \includegraphics[scale=0.3]{geometrystage}
\end{figure}

This is responsible for the majority of operations with polygons and their vertices

\subsubsection{Vertex Processing}
The input vertex stream, composed of arbitrary vertex attributes, is transformed into a stream of vertices, composed of their clip space coordinates and additional user-define attributes, mapped onto the screen by the vertex shader.

\begin{figure}[htb!]
  \centering
  \caption{Possible pre-combinations of transformation matrices and their common names.}
  \includegraphics[scale=0.3]{inputvertextooutput}
\end{figure}

\subsubsection{Vertex Post-Processing}
This involves clipping, projection, and viewport transformation, which could be implemented to occur in any order.

\subsubsection{Geometry Shader}
This is an optional stage between the vertex shader and the fragment shader.

Unlike the vertex shader, the geometry shader has full knowledge of the primitive it is working on; it has access to each vertex of the primitive, including adjacency information.
It can also be used to generate primitives dynamically, such as in growing plants with procedural geometry.

\subsection{Rasterization Stage}
\begin{figure}[htb!]
  \centering
  \caption{The rasterization stage.}
  \includegraphics[scale=0.3]{rasterizationstage}
\end{figure}

Primitives are sampled into pixels on screen; discrete fragments are created from continuous surfaces.
The raster points are also called fragments; each fragment corresponds to one pixel in the frame buffer, and this corresponds to one pixel of the screen.

This involves:
\begin{itemize}
  \item \textbf{Primitive assembly} - backface culling and setup of the primitive for traversal.
  \item \textbf{Primitive traversal} (scan conversion) - sampling and interpolation of vertex attributes (e.g.\ depth, colour).
  \item \textbf{Fragment shading} - computing fragment colours based on textures, lighting calculations, etc.
  \item \textbf{Fragment merging} - composing the final pixel values from fragments over the same pixel; depth tests; blending\dots.
\end{itemize}

Different rules for rasterization are applied for each primitive type, this is called a \textit{fill convention}.

For polygons, we rasterize if the pixel centre is contained in the polygon, and if a pixel is on the edge, then only one is rasterized.

\subsection{Display Stage}
\begin{itemize}
  \item Gamma correction.
  \item Digital to analog conversion (historically).
  \item Digital scan-out, HDMI encryption, etc.
\end{itemize}

\subsubsection{Display Format}
The frame buffer pixel can be formatted as RGBA or as an index into a table of colours (obsolete).

\subsubsection{Functionality vs. Frequency}
\begin{itemize}
  \item \textbf{Geometry processing} (per-vertex):
    \begin{itemize}
      \item Transformation and lighting (T\&L).
      \item Historically floating point, complex operations.
      \item Millions of vertices per second.
      \item Today - \textbf{vertex shader}.
    \end{itemize}
  \item \textbf{Fragment processing} (per-fragment):
    \begin{itemize}
      \item Blending, texture combination.
      \item Historically fixed point and limited operations.
      \item Billions of fragments.
      \item Today - \textbf{fragment shader}.
    \end{itemize}
\end{itemize}

\subsection{Architectural Overview}
\begin{figure}[htb!]
  \centering
  \caption{Overview of the architecture.}
  \includegraphics[scale=0.3]{architecture}
\end{figure}

\begin{itemize}
  \item The graphics hardware is a shared resource.
  \item \textbf{User Mode Driver} (UMD) - prepares command buffers for the hardware.
  \item \textbf{Graphics Kernel Subsystem} - schedules access to the hardware.
  \item \textbf{Kernel Model Drive} (KMD) - submits command buffers to the hardware.
\end{itemize}

\section{Graphics APIs}
\subsection{What is \textit{OpenGL}?}
\begin{itemize}
  \item Low-level graphics API specification.
    \begin{itemize}
      \item Interface is platform independent but the implementation is platform dependent.
      \item Defines an abstract rendering device and functions to operate it.
      \item Can draw, but no concept of permanent objects.
    \end{itemize}
  \item Platform provides the implementation, as part of the graphics driver or the runtime library on top of the driver.
  \item Initialization through platform specific API.
  \item State machine for high efficiency.
\end{itemize}

\subsection{\textit{GLSL}}
\textit{GLSL} is the shader language for \textit{OpenGL} and is used to define parts of the pipeline using own programs.
With the exception of rasterization, all processing steps of the graphics card can be programmed directly.

The application developer passes the shader source code and all additional variables and constants for each shader type to the \textit{OpenGL} driver.
The driver compiles and links the shaders to a shader program.

Each primitive sent by the application will pass through the shaders contained in the shader program in the following order:
\begin{enumerate}
  \item \textbf{Vertex Shader}:
    \begin{itemize}
      \item Executed once for each vertex.
      \item Shader only has access to vertex, but not neighbouring vertices or topology.
    \end{itemize}
  \item \textbf{Tessellation Shader}:
    \begin{itemize}
      \item An area (triangle or square) is divided into smaller areas.
    \end{itemize}
  \item \textbf{Geometry Shader}:
    \begin{itemize}
      \item Primitives can be created from an existing primitive (point, line, triangle).
    \end{itemize}
  \item \textbf{Fragment Shader}:
    \begin{itemize}
      \item Executed once for each fragment.
      \item The colour for the corresponding fragment is calculated.
    \end{itemize}
\end{enumerate}

\section{Illumination and Shading}
When we look at a point on an object, the colour and shading intensity perceived depend on various characteristics of the object and light sources that illuminate it.
\begin{itemize}
  \item \textbf{Object properties}:
    \begin{itemize}
      \item Position relative to the light sources.
      \item Surface normal vector.
      \item Albedo (ability to absorb light energy) of the surface, or reflectivity of the surface.
    \end{itemize}
  \item \textbf{Light source properties}:
    \begin{itemize}
      \item Intensity of emitted light.
      \item Distance to the point on the surface.
    \end{itemize}
\end{itemize}

\subsection{Radiometry}
\begin{defn}
  \textbf{Radiation flux} is the incident radiant energy over unit time.
\end{defn}

\begin{defn}
  \textbf{Radiance} is the radiant flux per unit solid angle per unit projected area.
  \[
    L(\omega) = \frac{d^2 \Phi}{\cos \theta dAd\omega} 
  \]
\end{defn}

\begin{defn}
  \textbf{Irradiance} is the differential flux falling onto differential area.
  \[
    E = \frac{d\Phi}{dA}
  \]
\end{defn}

\begin{defn}
  \textbf{Reflection} is the process by which electromagnetic flux incident on a surface leaves the surface without a change in frequency.
\end{defn}

\begin{defn}
  \textbf{Reflectance} is a fraction of the incident flux that is reflected.
\end{defn}

\subsection{Reflectance}
\begin{figure}[htb!]
  \centering
  \caption{Irradiance and radiance.}
  \includegraphics[scale=0.3]{brdf}
\end{figure}
\begin{defn}
  The \textbf{Bidirectional Reflectance Distribution Function} (BRDF) defines how light is reflected:
  \[
    f_r(\theta_i, \phi_i, \theta_r, \phi_r) = \frac{dL_r (\theta_r, \phi_r)}{dE_i (\theta_i, \phi_i)}  
  \]
\end{defn}

\subsubsection{Isotropic BRDFs}
Rotation along the surface normal does not change reflectance, so:
\[
  f_r(\theta_i, \phi_i, \theta_r - \phi_r) = f_r(\theta_i, \theta_r, \phi_d) = \frac{dL_r (\theta_r, \phi_d)}{dE_i (\theta_i, \phi_d)}  
\]

\subsubsection{Anisotropic BRDFs}
These are concerned with surfaces with strongly oriented microgeometry elements, e.g.\ brushed metals, hair, fur, cloth.

\subsubsection{Properties}
\begin{itemize}
  \item \textbf{Non-negativity}: $f_r(\theta_i, \phi_i, \theta_r, \phi_r) \geq 0$.
  \item \textbf{Energy Conservation}: $\int_\Omega f_r(\theta_i, \phi_i, \theta_r, \phi_r) d\mu(\theta_r, \phi_r) \leq 1 \text{ for all } (\theta_i, \phi_i)$.
  \item \textbf{Reciprocity}: $f_r(\theta_i, \phi_i, \theta_r, \phi_r) = f_r(\theta_r, \phi_r, \theta_i, \phi_i)$.
\end{itemize}

\subsubsection{BSSRDF}
The bidirectional scattering-surface distribution function is used with surfaces which have layers, e.g.\ skin.
Light may be absorbed by one layer and reflected by another.

\subsection{Ideal Diffuse Reflectance}
We assume the surface reflects equally in all directions, an ideal diffuse surface would be a very rough surface at the microscopic level, e.g.\ chalk, clay.

Then the value of the BRDF value would be constant throughout:
\[
  L_r(\omega_r) = \int_\Omega f_r(\omega_i, \omega_r) dE_i(\omega_i) = f_r \int_\Omega dE_i(\omega_i) = f_r E_i
\]

Ideal diffuse reflectors reflect light according to \textit{Lambert's cosine law} - this means that the greater the angle of the light direction to the surface normal (up to orthogonal), the less is reflected. 

We further model the reflectance as:
\[
  L(\omega_r) = k_d (\bm{n} \cdot \bm{l}) \frac{\Phi_s}{4\pi d^2} 
\]
where
\begin{itemize}
  \item $k_d$ is the diffuse reflection coefficient.
  \item $n$ is the surface normal.
  \item $l$ is the light direction.
\end{itemize}

We use $\max ((\bm{n} \cdot \bm{l}), 0)$ to avoid a negative dot product value when the vectors are facing away from each other, and must normalize the vectors before applying the dot product.

\subsection{Ideal Specular Reflectance}
Here, reflection is only at a mirror angle.
\begin{itemize}
  \item View dependent.
  \item Microscopic surface elements oriented in the same direction as the surface itself.
\end{itemize}

\begin{figure}[htb!]
  \centering
  \caption{Snell's Law.}
  \includegraphics[scale=0.3]{snell}
\end{figure}

In \textit{Snell's Law}, the incoming ray, the surface normal, and the reflected ray all lie in a common plane.

\subsection{Non-Ideal Reflectors}
\textit{Snell's Law} only applies to ideal mirror reflectors, real materials tend to deviate significantly from ideal mirror reflectors and they are not ideal diffuse surfaces either.

\subsubsection{Simple Empirical Model}
\begin{itemize}
  \item Most of the reflected light travels in the direction of the ideal ray.
  \item We expect some to reflect slightly offset from the ideal.
  \item As we move further away in the angular sense from the reflected ray, less light is reflected.
\end{itemize}

\subsubsection{Surface Characteristics}
\begin{itemize}
  \item \textbf{Perfectly matte surface} - reflected intensity same in all directions.
  \item \textbf{Slightly specular (shiny) surface} - slightly higher intensity in the reflected direction.
  \item \textbf{Highly specular (shiny) surface} - high intensity in the reflected direction.
  \item \textbf{Perfect mirror} - all light is re-admitted in the reflected direction.
\end{itemize}

\subsection{Phong Model}

\begin{figure}[htb!]
  \centering
  \caption{The Phong model.}
  \includegraphics[scale=0.3]{phong}
\end{figure}
The light reflected depends on the angle between the ideal reflection direction and the viewer direction $\alpha$.

\[
  L(\omega_r) = k_s(\cos \alpha)^q \frac{\Phi_s}{4\pi d^2} = k_s (\bm{v} \cdot \bm{r})^q \frac{\Phi_s}{4\pi d^2} 
\]
where:
\begin{itemize}
  \item $k_s$ is the specular reflection coefficient.
  \item $q$ is the specular reflection exponent.
  \item $\bm{r} = 2(\bm{n} \cdot \bm{l}) \bm{n} - \bm{l}$.
\end{itemize}

Higher values of $q$ result in greater, more centred reflections.

\subsubsection{Blinn-Phong Variation}
\begin{figure}[htb!]
  \centering
  \caption{The Blinn-Phong variation.}
  \includegraphics[scale=0.3]{blinnphong}
\end{figure}
We use the halfway vector $\bm{h}$ between $\bm{l}$ and $\bm{v}$:
\[
  \bm{h} = \frac{\bm{l} + \bm{v}}{\lVert \bm{l} + \bm{v} \rVert} 
\]
leading to:
\[
  L(\omega_r) = k_s(\cos \beta)^q \frac{\Phi_s}{4\pi d^2} = k_s (\bm{n} \cdot \bm{h})^q \frac{\Phi_s}{4\pi d^2} 
\]

\subsection{Ambient Illumination}
This represents the reflection of all indirect illumination - a hack.
\[
  L(\omega_r) = k_a 
\]
where $k_a$ is the ambient coefficient.

\subsection{Phong Illumination Model}
We sum up the three components, diffuse reflection + specular reflection + ambient:
\[
  L(\omega_r) = k_a + (k_d (\bm{n} \cdot \bm{l}) + k_s (\bm{v} \cdot \bm{r})^q) \frac{\Phi_s}{4\pi d^2} 
\]

\subsubsection{Inverse Square Law and Heuristic Law}
Since light falls off according to the inverse square law, we multiply by $\frac{\Phi_s}{4\pi d^2}$, where $d$ is the distance from the light source to the object.

Although this is physically correct, it does not produce the best results.
We often use $\frac{\Phi_s}{4\pi(d + s)}$, where $s$ is a heuristic constant.

\subsection{Shading}
There are three levels at which shading can be applied:
\begin{itemize}
  \item \textbf{Flat shading}.
  \item \textbf{Gouraud shading}.
  \item \textbf{Phong shading}.
\end{itemize}
where the latter are \textbf{interpolation shading}.

\subsubsection{Flat Shading}

\begin{itemize}
  \item Each polygon is shaded uniformly over its surface.
  \item Shade is computed by taking a point in the centre and the surface normal vector.
  \item Usually only defuse and ambient components used.
\end{itemize}

\subsubsection{Interpolation Shading}
\begin{figure}[htb!]
  \centering
  \caption{Calculating the shades at the edge.}
  \includegraphics[scale=0.3]{interpolationedge}
\end{figure}

\begin{figure}[htb!]
  \centering
  \caption{Calculating the internal shades.}
  \includegraphics[scale=0.3]{interpolationinternal}
\end{figure}

\begin{itemize}
  \item Compute an independent shade value at each point.
  \item Done quickly by interpolation:
    \begin{enumerate}
      \item Compute a shade value at each vertex.
      \item Interpolate to find the shade value at the boundary.
      \item Interpolate to find the shade values in the middle.
    \end{enumerate}
\end{itemize}


\subsubsection{Interpolating over Polygons}
We can interpolate over groups of polygons to create the impression of a smooth surface.
The idea is to create at each vertex an averaged intensity from all the polygons that meet at that vertex.

To compute an average normal vector at a vertex:
\[
  n_ave = (n_1 + n_2 + n_3 + n_4) / 4
\]
where $n_i$ are the normal vectors of the polygons that meet at the vertex.

\subsubsection{Smooth Shading}
This requires per-vertex normals.
\begin{itemize}
  \item \textbf{Gouraud Shading}:
    \begin{itemize}
      \item Interpolate colour across triangles.
      \item Fast, supported by most graphics accelerator cards.
      \item Cannot model specular components accurately as we do not have the normal vector at each point on a polygon.
    \end{itemize}
  \item \textbf{Phong Shading}:
    \begin{itemize}
      \item Interpolate normals across triangles.
      \item More accurate specular modelling, but slower.
    \end{itemize}
\end{itemize}

\subsubsection{Interpolation of the 3D Normals}
We may express any point in parametric form:
\[
  \bm{P} = \bm{V}_1 + \mu_1 (\bm{V}_2 - \bm{V}_1) + \mu_2 (\bm{V}_3 - \bm{V}_1)
\]

The average normal vector at the same point may be calculated as the vector $\bm{a}$:
\[
  \bm{a} = \bm{n}_1 + \mu_1 (\bm{n}_2 - \bm{n}_1) + \mu_2 (\bm{n}_3 - \bm{n}_1)  
\]
then:
\[
  \bm{n}_{average} = \bm{a} / \lvert \bm{a} \rvert 
\]

Interpolation calculations may be done in either 2D or 3D, but for specular reflections the calculation of the reflected vector and viewpoint vector must be done in 3D.

\section{Colour}
Lasers are light sources that contain a single wavelength (or a narrow band of).
In practice, light is made up of a mixture of many wavelengths with an energy distribution.

Human colour vision is based on three cone cell types which respond to light energy in different bands of wavelength.
These bands overlap, which implies that colours do have a unique energy distribution.
Colours which are a distribution over all wavelengths can be matched by mixing three: \textbf{RGB}.

\subsection{Colour Matching}
Given any colour light source, we can try to match it with a mixture of three light sources:
\[
  X = rR + gG + bB 
\]
where $R, G, B$ are pure light sources, and $r, g, b$ are their intensities - for simplicity we drop $R, G, B$.

\subsection{Subtractive Matching}
Not all colours can be matched with a given set of light sources.
We can add light to the colour we are trying to match:
\[
  X + r = g + b 
\]

With this, we can match all colours.

\subsection{The CIE Diagram}
\begin{figure}[htb!]
  \centering
  \caption{CIE diagram.}
  \includegraphics[scale=0.3]{cie}
\end{figure}
The CIE diagram is a standard normalised representation of colour.
Given three light sources, and allowing ourselves subtractive matching, we can mix them to match any given colour.

\subsubsection{Normalized Colours}
\begin{figure}[htb!]
  \centering
  \caption{Normalized colours.}
  \includegraphics[scale=0.3]{normalizedcolours}
\end{figure}
We normalize the ranges found to $[0\dots1]$ to avoid the negative signs.
We then normalize the colours so that the three components sum to $1$:
\begin{align*}
  x &= r / (r + g + b) \\
  y &= g / (r + g + b) \\
  z &= b / (r + g + b) = 1 - x - y
\end{align*}

\subsubsection{Convex Shape}
The pure colours (coherent $\lambda$) are round the edge of the CIE diagram.
The shape must be convex (convex set) since any blend (interpolation) of pure colours should create a colour in the visible region.

Note that the line joining red and purple has no pure equivalent, the colours can only be created by blending.

\subsubsection{Intensities}
Since the colours are all normalized, there is no representation of intensity.
By changing intensity, different colours can be seen.

\subsubsection{White Point}
When the three components are equal, the colour is white.
This is seen at point $(0.33, 0.33)$.

\subsubsection{Saturation}
Pure colours are fully saturated, these correspond to the colours around the edge of the horseshoe.
Saturation of an arbitrary point is the ratio of its distance to the white point over the distance of the white point to the edge.

\subsubsection{Complement Colour}
The complement of a fully saturated colour is the point diametrically opposite through the white point.
A colour added to its complement gives us white.

\begin{figure}[htb!]
  \centering
  \caption{Using the CIE diagram to find properties of a colour.}
  \includegraphics[scale=0.3]{cie2}
\end{figure}

\subsection{Subtractive Primaries}
When printing colour, we use a subtractive representation.
Inks absorb wavelengths from incident light so they subtract components to create colour.
The subtractive primaries are magenta, cyan, and yellow.

\subsection{Reproducible Colours}
Colour monitors are based on the output of three different light emitting phosphors or diodes.
The nominal position of these are:
\begin{center}
  \begin{tabular}{c c c c c}
    & x & y & z \\
    Red & 0.628 & 0.346 & 0.026 \\
    Green & 0.268 & 0.588 & 0.144 \\
    Blue & 0.150 & 0.07 & 0.780 
  \end{tabular}
\end{center}

The monitor RGB representation is related to CIE colours by:
\[
  (x, y, z) =
  \begin{pmatrix}
    0.628 & 0.268 & 0.15 \\
    0.346 & 0.588 & 0.07 \\
    0.026 & 0.144 & 0.78
  \end{pmatrix}
  \begin{pmatrix}
    R \\ G \\ B
  \end{pmatrix}
\]

\subsection{HSV Colour Representation}
RGB and CIE are practical representations and do not relate to how we perceive colours.
\begin{itemize}
  \item \textbf{Hue} - corresponds to pure colour.
  \item \textbf{Saturation} - proportion of pure colour.
  \item \textbf{Value} - the brightness (intensity).
\end{itemize}

\begin{align*}
  r = g &= b & H &\text{ is undefined} \\
  \min(r, g, b) &= b & H &= \frac{120(g - b)}{(r - b) + (g - b)} \\
  \min(r, g, b) &= r & H &= 120 + \frac{120(b - r)}{(g - r) + (b - r)} \\
  \min(r, g, b) &= g & H &= 240 + \frac{120(r - g)}{(r - g) + (b - g)}
\end{align*}

\[
  S = \frac{\max(r,g,b) - \min(r, g, b)}{\max(r, g, b)} 
\]

\[
  V = \max(r, g, b) 
\]

In the RGB system, each point is treated as a mixture of pure colour and white.
The smallest component is taken as white and the ratio of the other components (after subtraction of the white component) is the pure colour.

The pure colours however are not coherent wavelengths as in the CIE diagram.

\subsection{Alpha Channels}
The $\alpha$ value is the attenuation of the intensity, allowing greater flexibility in colour representation and avoids truncation errors at low intensity, or for masking certain parts of an image.
\begin{align*}
  \alpha &= 0 && \text{transparent} \\
  0 < \alpha &< 1 && \text{semi-transparent} \\
  \alpha &= 1 && \text{opaque}
\end{align*}

\[
  C = (R, G, B) \rightarrow C = (\alpha r, \alpha g, \alpha b, \alpha) 
\]

\section{Texture Mapping}
Textures enhance the visual appearance of a graphics scene without requiring greater effort in scene design, e.g.\ using a polygon for individual bricks and using one polygon with a repeating brick pattern.

Textures can be defined as:
\begin{itemize}
  \item One-dimensional functions.
  \item Two-dimensional functions.
  \item Raster images (texels).
  \item Three-dimension functions.
\end{itemize}

\subsection{Procedural Textures}
\[
  F(\bm{p}) \rightarrow \text{ colour} 
\]

\begin{itemize}
  \item Algorithm is used to calculate each colour rather than directly stored data.
  \item Non-intuitive.
  \item Difficult to match a specific texture that already exists in the real world.
\end{itemize}

\subsection{The Concept of Texture Mapping}
\begin{figure}[htb!]
  \centering
  \caption{Spaces and functions involved in texture mapping.}
  \includegraphics[scale=0.3]{texturemapping}
\end{figure}
We limit the axis of the texture image to $[0, 1]$.

For each triangle or polygon in the model, we establish a corresponding region in the texture.
During rasterization, we interpolate the coordinate indices into the texture map.

\subsection{Parametrization}
There are several ways in which we could map an image into the model:
\begin{itemize}
  \item \textbf{Planar mapping}:
    \begin{itemize}
      \item One of the coordinates of the model is dumped.
      \item Only looks good from the front.
    \end{itemize}
  \item \textbf{Cylindrical and Spherical Mapping}:
    \begin{itemize}
      \item Compute angles between the vertex and object center.
      \item Compare to polar/spherical coordinate systems.
      \item Singularity problems.
    \end{itemize}
  \item \textbf{Box Mapping}:
    \begin{itemize}
      \item A separate texture for each plane.
      \item Used mainly for environment mapping.
    \end{itemize}
  \item \textbf{Manual Mapping}:
    \begin{itemize}
      \item CAD software.
      \item ``Unwrapping''.
    \end{itemize}
\end{itemize}

All mappings have distortions and singularities - these often need to be fixed manually.

\subsection{Texture Addressing}
When we map to a coordinate of the texture image outside $[0, 1]$, we have several texture addressing modes available, see figure \ref{fig:addressing}.

\begin{figure}[htb!]
  \centering
  \caption{Texture addressing modes.}
  \label{fig:addressing}
  \includegraphics[scale=0.3]{addressing}
\end{figure}

To aid in repeated addressing, we may introduce seamless tiling in the source texture in order to improve the look of the result.
We could use \textit{texture synthesis} to generate tiling textures.

\subsection{Texture Coordinates}
At each vertex, we specify a texture coordinate ($(0, 0) \rightarrow (1, 1)$), then linearly interpolate the values in screen space.
This is the same as Gouraud shading, but for texture coordinates.

However, this can cause distortion in perspective projection.
This is because the linear combination of points is not preserved - the equal distances in 3D space do not map to equal distances in screen space, the \textbf{linear interpolation is different}, see figure \ref{fig:distortion}.

\begin{figure}[htb!]
  \centering
  \caption{3D space interpolation against screen space interpolation.}
  \label{fig:distortion}
  \includegraphics[scale=0.3]{distortion}
\end{figure}

\subsubsection{Perspective Correct Interpolation}
\begin{figure}[htb!]
  \centering
  \caption{The points involved in perspective correct interpolation.}
  \includegraphics[scale=0.3]{perspectivecorrect}
\end{figure}
To find the texture parameter $t$ at point $q$:
\[
  t_q = \frac{\text{lerp}\left(\frac{t_p}{z_p}, \frac{t_r}{z_r} \right)} {\text{lerp}\left(\frac{1}{z_p}, \frac{1}{z_r} \right)}
\]
where:
\begin{itemize}
  \item $\text{lerp}$ is linear interpolation.
  \item $t_i$ is the texture parameter at point $i$.
  \item $z_i$ is the $z$ coordinate at point $i$.
  \item Assuming the image plane is at $z = 1$.
\end{itemize}

\subsubsection{Bilinear Mapping}
\begin{figure}[htb!]
  \centering
  \caption{Bilinear texture mapping.}
  \includegraphics[scale=0.3]{bilinearmap}
\end{figure}
An alternative method is to use bilinear texture mapping.
\begin{align*}
  \bm{p} &= \alpha\bm{a} + \beta\bm{e}
  \bm{e} &= \bm{b} + \alpha(\bm{c} - \bm{b})
\end{align*}
so:
\[
  \bm{p} = \alpha\bm{a} + \beta\bm{b} + \alpha\beta(\bm{c} - \bm{b})
\]

The second order term means that straight lines in the texture may become curved.
However if the mapping is to a parallelogram where $\bm{b} = \bm{c}$, then:
\[
  \bm{p} = \alpha\bm{a} + \beta\bm{b}
\]

\subsection{Additional Mappings}
\begin{itemize}
  \item \textbf{Bump Mapping}:
    \begin{itemize}
      \item Textures can be used to alter the surface normal of an object - the shading changes but the shape is kept the same.
      \item The texture map is treated as a single-valued height function.
      \item The partial derivatives tell us how to alter the true surface normal at each point, to make it appear deformed.
    \end{itemize}
  \item \textbf{Displacement Mapping}:
    \begin{itemize}
      \item We can also use a texture map to actually move the surface point.
      \item The geometry must be displaced before visibility is determined.
    \end{itemize}
  \item \textbf{Environment Maps}:
    \begin{itemize}
      \item We can simulate reflections by using the direction of the reflected ray to index a spherical texture map at ``infinity''.
      \item We assume all reflected rays begin from the same point.
    \end{itemize}
\end{itemize}

\section{Rasterization, Visibility and Anti-Aliasing}

\subsection{Coordinate Systems}
\begin{figure}[htb!]
  \centering
  \caption{Trilinear and Barycentric coordinates.}
  \includegraphics[scale=0.3]{tribary}
\end{figure}
In \textbf{trilinear coordinates}, we project each vertex of the triangle onto the opposite edge and measure the distance to a particular point to calculate the coordinates of the point.

In \textbf{barycentric coordinates}, we use the ratios of the subtriangles formed by the vertices of the triangle and the point to calculate the coordinates of the point.A

\subsection{Barycentric Coordinates}
We use vertices $a, b, c$ to specify the three points of a triangle.
These three points determine a plane, and the vectors $b - a$ and $c - a$ form a basis for this plane.

Then any point $p$ in the plane can be specified:
\begin{align*}
  p &= a + \beta (b - a) + \gamma(c - a) 
  &= (1 - \beta - \gamma) a + \beta b + \gamma c
  &= \alpha a + \beta b + \gamma c
\end{align*}
\[
  p(\alpha, \beta, \gamma) = \alpha a + \beta b + \gamma c
\]
where $\alpha, \beta, \gamma$ are called barycentric coordinates.

We can have two types of barycentric coordinates:
\begin{itemize}
  \item \textbf{Homogeneous barycentric coordinates} are normalized so that $\alpha + \beta + \gamma = \text{ area of triangle}$.
  \item \textbf{Areal coordinates} or \textbf{absolute barycentric coordinates} are normalized by the area of the original triangle, $\alpha + \beta + \gamma = 1$.
\end{itemize}

\subsubsection{Properties}
\begin{itemize}
  \item Barycentric coordinates describe a point $p$ as an affine combination of the triangle vertices:
    \begin{align*}
      p(\alpha, \beta, \gamma) = \alpha a + \beta b + \gamma c & & \alpha + \beta + \gamma = 1
    \end{align*}
  \item For any point $p$ inside the triangle $(a, b, c)$:
    \begin{align*}
      0 < \alpha < 1 && 0 < \beta < 1 && 0 < \gamma < 1
    \end{align*}
  \item Points on the edge have one coefficient as $0$.
  \item At the vertices, two coordinates are $0$, the remaining is $1$.
\end{itemize}

\subsubsection{Signed Distances}
\begin{figure}[htb!]
  \centering
  \caption{The possible positions of a point for different values of coordinate $\beta$.}
  \includegraphics[scale=0.5]{signeddistances}
\end{figure}
Let $p = \alpha a + \beta b + \gamma c$, each coordinate is the signed distance from $p$ to the line through the triangle edge.

The signed distance can be computed by evaluating implicit line equations of the edges.

\subsubsection{Implicit Equation for Lines}
Recall $f(x, y)$:
\begin{itemize}
  \item Points with $f(x, y) = 0$ are on the line.
  \item Points with $f(x, y) \neq 0$ are not on the line.
\end{itemize}

The general form is $Ax + By + C = 0$.
The implicit line through two points $(x_a, y_a)$ and $(x_b, y_b)$:
\[
  (y_a - y_b) x + (x_b - x_a) y + x_a y_b - x_b y_a = 0 
\]

For a given point $p$, we want to compute its barycentric coordinate $\beta$.
We know that if $f(x, y) = 0$, then $kf(x, y) = 0$, then we need to choose a $k$ such that:
\[
  kf_{ac} (x, y) = \beta 
\]

We know $\beta = 1$ at point $b$, then if $kf_{ac}(x_b, y_b) = 1$, so $k = \frac{1}{f_{ac}(x_b, y_b)}$:
\[
  \beta = \frac{f_{ac}(x, y)}{f_{ac}(x_b, y_b)}
\]

In general, we have:
\begin{align*}
  \alpha = \frac{f_{bc}(x, y)}{f_{bc}(x_a, y_a)} &&
  \beta = \frac{f_{ac}(x, y)}{f_{ac}(x_b, y_b)} &&
  \gamma = 1 - \alpha - \beta
\end{align*}
Given a point in Cartesian coordinates $(x, y)$, we can calculate its barycentric coordinates $(\alpha, \beta, \gamma)$.

\[
  \begin{pmatrix}
    x_a & x_b & x_c \\
    y_a & y_b & y_c \\
    1 & 1 & 1
  \end{pmatrix}
  \begin{pmatrix} \alpha \\ \beta \\ \gamma \end{pmatrix}
  =
  \begin{pmatrix} x \\ y \\ 1 \end{pmatrix}
\]

\subsubsection{Conversion to Trilinear}
\[
  P_t(t_1, t_2, t_3) = P_b(t_1 a, t_2 b, t_3 c) 
\]
where $a, b, c$ are the side lengths of the triangle.

\subsection{Triangle Rasterization}
A simple method of generating fragments for a rectangle is checking $(\alpha, \beta, \gamma)$ and checking if they are all within the bounds of the rectangle.

With one triangle, things are simple as pixels never overlap.

With more than one, fragments might overlap in screen space.
It may be a simple case where one is on top of the other, or more complicated where they may intersect with each other - we need to decide what to draw first.

\subsubsection{Painter's Algorithm}
\begin{enumerate}
  \item Sort triangles on $z$ values (depth) in eye space.
  \item Draw triangles from back to front.
\end{enumerate}

There is an issue with correctness when handling intersections or overlaps in a cycle - we could solve this with splitting triangles but this is ugly and expensive.
Another issue is in the efficiency of sorting all triangles.

\subsubsection{The Depth Buffer (Z-Buffer)}
Instead, we can perform hidden surface removal per-fragment:
\begin{enumerate}
  \item Each fragment gets a $z$ value in the screen space.
  \item We keep only the fragment with the smallest $z$ value.
\end{enumerate}
As many fragments may map to the same pixel location so we need to track their $z$-values using a $z$-buffer.
This is a 2D buffer with the same size as the image, containing all the current depth values of the pixels.

\subsection{Anti-Aliasing}
One major problem with rasterization are alias effects (e.g.\ jagged edges), these are caused by undersampling and can cause unreal visual artefacts.


\subsubsection{Supersampling}
The most successful solution to this problem is to apply a degree of blurring to the boundary to reduce the effect, the most successful technique is \textbf{supersampling}.

Compute the picture at a \textbf{higher resolution} than the display area and average to find the pixel values, thus blurring boundaries but leaving coherent areas of colour unchanged.

While supersampling works well for scenes made up of filled polygons, it does not work for line drawings and has high computational costs.

\subsubsection{Convolution Filtering}
The more common and faster method is to use a \textbf{filter to blur the image}, taking an average over a small region around each pixel to change the resultant pixel.

Taking a local average can have undesirable effects so we normally use a weight average, putting more weight on pixels in the centre of the region.

The disadvantages to this method are that while visual appearance is enhanced, the image quality is degraded.

\subsubsection{Textures}
When we identify a point in the texture map, we return an average of texture map around the point.

\section{Ray Tracing}
In global illumination, a surface point receives light after the light rays have interacted with other objects in the scene.
We need to consider shadows, refraction, reflection and transmission.

A simple method which is still a local illumination model is to cast one ray per pixel of the image.
For each intersection with an object, trace another ray to the light and draw a shadow if this intersects with another object.

\subsection{Recursive Ray Casting}
For a global illumination model, we again cast one ray per pixel of the image and cast shadow rays to the light sources.
However we also consider mirrors and transparency, tracing a ray for reflections and transmissions.
As we trace these additional rays, they may then trace their own rays, leading to a recursive process.

Since the process can continue infinitely, we can decide to use some stopping criteria:
\begin{itemize}
  \item Recursion depth - stop after a number of bounces.
  \item Ray contribution - stop if the contribution from reflection or transmission becomes too small.
\end{itemize}

\subsubsection{Primary Rays}
For each ray we need to test which objects are intersecting the ray.
If the object has an intersection with the ray, we calculate the distance between the intersection point and the viewpoint.
If the ray has more than one intersection, the smallest distance identifies the visible surface.

The primary rays are thus the rays from the viewpoint to the nearest intersection point.
Local illumination is computed as before:
\[
  L = k_a + (k_d (\bm{n} \cdot \bm{l}) + k_s(\bm{v} \cdot \bm{r})^q)I_s 
\]

\subsubsection{Secondary Rays}
Secondary rays are rays originating at intersection points, these can be caused by being:
\begin{itemize}
  \item Reflected off the surface in the direction of reflection.
  \item Transmit through transparent materials in the direction of refraction.
  \item Shadow rays.
\end{itemize}

\subsubsection{Final Equation}
\[
  L = k_a + (k_d (\bm{n} \cdot \bm{l}) + k_s(\bm{v} \cdot \bm{r})^q)I_s + k_{reflected} L_{reflected} + k_{refracted}L_{refracted}
\]

\subsection{Precision Problems}
\label{sec:epsilon}
The origin of secondary rays is often below the surface of objects due to calculation imprecision.
This results in the surface area shadowing itself as the ray intersects with the surface.

To solve this, we check if the distance between the surface and the intersection point is within some epsilon tolerance:
\begin{itemize}
  \item If $ \lvert \mu \rvert < \epsilon$, the point is on the surface.
  \item Otherwise, the point is inside/outside the surface.
\end{itemize}

If the point is on the surface, we move the intersection point by $\epsilon$ along the surface normal so that it is outside the object.

\subsection{Mirror Reflection}
To cast the ray:
\begin{enumerate}
  \item Compute the mirror contribution.
  \item Cast the ray in the direction symmetric with respect to the normal.
  \item Multiply by the reflection coefficient.
\end{enumerate}

To calculate the illumination as a result of reflections:
\begin{itemize}
  \item Calculate the direction of the secondary ray at the intersection of the primary ray with the object.
  \item 
    \[
      \bm{v}' = \bm{v} - (2 \bm{v} \cdot \bm{n}) \bm{n} 
    \]
    where:
    \begin{itemize}
      \item $\bm{n}$ is the unit surface normal.
      \item $\bm{v}$ is the direction of the primary ray.
      \item $\bm{v}'$ is the direction of the secondary ray as a result of reflections.
    \end{itemize}
\end{itemize}

\subsection{Transparency}
To compute the ray:
\begin{enumerate}
  \item Compute the transmitted contribution.
  \item Cast the ray in the refracted direction.
  \item Multiply by the transparency coefficient.
\end{enumerate}

\subsubsection{Refraction}
The angle of the refracted ray is determined by Snell's Law:
\[
  \eta_1 \sin(\phi_1) = \eta_2 \sin(\phi_2)
\]
where:
\begin{itemize}
  \item $\eta_1$ is the constant for medium 1.
  \item $\eta_2$ is the constant for medium 2.
  \item $\phi$ is the angle between the incident ray and the surface normal.
  \item $\phi$ is the angle between the refracted ray and the surface normal.
\end{itemize}

In vector notation:
\[
  k_1 (v \cdot n) = k_2 (v' \cdot n) 
\]

If the light passes from one medium to another whose index of refraction is low, then the angle of the refracted ray is greater than the angle of the incident ray - if the incident ray is large then the refracted ray angle will be greater than $90\degree$, so the ray is reflected rather than refracted.

\subsubsection{Fresnel Factor}
We tend to mix reflected and refracted light according to the Fresnel factor:
\[
  L = k_{fresnel} L_{reflected} + (1 - k_{fresnel}) L_{refracted} 
\]
The smaller the angle between the ray and the normal, the more that is transmit through the material.

Schlick's approximation:
\[
  k_{fresnel} (\theta) = k_{fresnel} (0) + (1 - k_{fresnel}(0)) (1 - (\bm{n} \cdot \bm{l}))^5
\]
where $k_{fresnel}(0)$ is the Fresnel factor at zero degrees, we can choose $0.8$ for stainless steel.

\subsection{Shadows}
We add $s$ to the equation:
\[
  L = k_a + s(k_d (\bm{n} \cdot \bm{l}) + k_s(\bm{v} \cdot \bm{r})^q)I_s + k_{reflected} L_{reflected} + k_{refracted}L_{refracted}
\]
where:
\[
  s =
  \begin{cases}
    0, & \text{if the light source is obscured} \\
    1, & \text{if the light source is not obscured}
  \end{cases}
\]

We have one shadow ray per intersection per point light source.

For softer shadows, we can have multiple shadow rays from an intersection to sample an area light source.

\subsection{Monte Carlo Ray Tracing}
With traditional reflections, we use one reflection per intersection.
To achieve glossy reflections, we can use \textbf{Monte Carlo ray tracing} - random reflection rays around the mirror direction:
\begin{enumerate}
  \item Cast a ray from the eye through each pixel (as before).
  \item Cast random rays from the visible point - accumulate radiance contribution.
  \item Recurse.
  \item From all intersection points of rays, send ray to the light.
  \item Trace only one secondary ray per recursion, but send many primary rays per pixel.
\end{enumerate}

\section{Intersection Calculations}
For each ray, we must calculate \textbf{all possible intersections} with each object inside the viewing volume, and we must find the \textbf{nearest intersection point}.

\subsection{Rays}
Rays are parametric lines:
\[
  p(\mu) = p_0 + \mu d 
\]
where:
\begin{itemize}
  \item $p_0$ is the origin.
  \item $d$ is the direction.
\end{itemize}

The coordinates of any point along each primary ray are given by $p = p_0 + \mu d$.
We can obtain $d$ from a point on the line $p_v$:
\[
  d = \frac{p_0 - p_v}{\lvert p_0 - p_v \rvert} 
\]

The viewing ray can be parametrized by $\mu$:
\begin{itemize}
  \item $\mu > 0$ denotes the part of the ray behind the viewing plane.
  \item $\mu < 0$ denotes the part of the ray in front of the viewing plane.
  \item For any visible intersection point, $\mu > 0$.
\end{itemize}

\subsection{Spheres}
\begin{figure}[htb!]
  \centering
  \caption{Intersection of a sphere.}
  \includegraphics[scale=0.5]{sphereintersection}
\end{figure}
For any point on the surface of the sphere $\lvert q - p_s \rvert ^2 - r^2 = 0$, where
$r$ is the radius of the sphere:
\[
  \mu = -d \cdot \Delta p \pm \sqrt{(d \cdot \Delta p) ^2 - \lvert \Delta p \rvert ^2 + r^2} 
\]

\begin{itemize}
  \item If we have no solutions $\mu$, then the ray does not intersect.
  \item If we have two solutions $\mu_1 < \mu_2$:
    \begin{itemize}
      \item $\mu_1$ is where the ray enters the sphere.
      \item $\mu_2$ is where the ray leaves the sphere.
    \end{itemize}
\end{itemize}

Recall section \ref{sec:epsilon}, we need to apply some epsilon tolerance.

\subsection{Cylinders}
A cylinder can be described by:
\begin{itemize}
  \item Two position vectors $p_1$ and $p_2$ describing the end points of the long axis of the cylinder.
  \item A radius $r$.
\end{itemize}
The axis of the cylinder can be written $\Delta p = p_1 - p_2$.
\[
  r^2 = \left( p_0 + \mu d - p_1 - \left( \frac{p_0 \cdot \Delta p + \mu d \cdot \Delta p - p_1 \cdot \Delta p}{\Delta p \cdot \Delta p} \right) \Delta p \right)^2
\]

Assuming that $\mu_1 \leq \mu_2$, we can determine two solutions:
\begin{align*}
  \alpha_1 = \frac{p_0 \cdot \Delta p + \mu_1 d \cdot \Delta p - p_1 \cdot \Delta p}{\Delta p \cdot \Delta p}
  &&
  \alpha_2 = \frac{p_0 \cdot \Delta p + \mu_2 d \cdot \Delta p - p_1 \cdot \Delta p}{\Delta p \cdot \Delta p}
\end{align*}
\begin{itemize}
  \item If $0 < \alpha_1 < 1$, the intersection is on the outside surface of the cylinder.
  \item If $0 < \alpha_2 < 1$, the intersection is on the inside surface of the cylinder.
\end{itemize}

\subsection{Primitives}
Objects are often described by geometric primitives; to test intersections of the ray with these primitives we must test whether the ray will intersect the plane defined by the primitive.

\subsubsection{Planes}
\[
  \mu = - \frac{(p_0 - p_1) \cdot n}{d \cdot n} 
\]
where:
\begin{itemize}
  \item $p_1$ is a point on the plane.
  \item $n$ is the normal of the plane.
\end{itemize}

\subsubsection{Triangles}
To calculate intersections, we must test 3 properties:
\begin{enumerate}
  \item Test whether the triangle is front facing:
    \[
      d \cdot n < 0 
    \]
    where $n$ is the normal to the plane of the triangle.
  \item Test whether the plane of the triangle intersects the ray:
    \[
      n = a \times b
    \]
    where $p_2 - p_1 = a$ and $p_3 - p_1 = b$.
  \item Test whether the intersection point is inside the triangle:
    \[
      q = \alpha a + \beta b
    \]
    The point is inside if:
    \begin{align*}
      0 \leq \alpha \leq 1 && 0 \leq \beta \leq 1 && \alpha + \beta \leq 1
    \end{align*}
    where we calculate $\alpha$ and $\beta$ with:
    \begin{align*}
      \alpha = \frac{(b \cdot b)(q \cdot a) - (a \cdot b)(q \cdot b)}{(a \cdot a)(b \cdot b) - (a \cdot b)^2}
      &&
      \beta = \frac{q \cdot b - \alpha(a \cdot b)}{b \cdot b}
    \end{align*}
\end{enumerate}

\section{Improving Ray Tracing}
While ray tracing is easy to implement and extends well to global illumination, it is slow due to the number of objects, rays, and ray-object intersections.

We want to reduce the number of ray-primitive intersections, this can be achieved by first checking for an intersection with a bounding region to allow early rejection.

\subsection{Regular Grids}
\begin{figure}[htb!]
  \centering
  \caption{Ray tracing using a regular grid.}
  \includegraphics[scale=0.5]{regulargrid}
\end{figure}

\begin{enumerate}
  \item Create a grid with resolution $(n_x, n_y, n_z)$.
  \item Insert primitives into the grid - if they overlap onto multiple cells then insert into multiple cells.
  \item Draw the ray - for each cell the ray crosses, check if it contains an intersection with the object(s) in it, and return the closest.
    \begin{itemize}
      \item Mark objects to prevent repeated computation.
      \item Do not return distant intersections (outside cell range), there may be closer intersections.
    \end{itemize}
\end{enumerate}

\subsection{Adaptive Grids}
\begin{figure}[htb!]
  \centering
  \caption{Adaptive grid and storage of primitives.}
  \includegraphics[scale=0.5]{adaptivegrid}
\end{figure}
Instead of static cell sizes, we can \textbf{subdivide cells} until each cell contains no more than $n$ elements, or a maximum depth $d$ is reached.

Primitives may be stored at the intermediate levels of the grid, or pushed to the lowest level.

\subsection{Binary Space Partition (BSP) Tree}
Like in a binary search tree, we recursively partition the space by planes such that every cell is a convex polyhedron.
We then trace rays by recursion on the tree - the construction enables simple front-to-back traversal.

\section{Spline Curves}
Splines are smooth curves defined from a small set of points called \textit{knots} or \textit{control points}:
\begin{itemize}
  \item \textbf{Interpolating splines} - curve passes through each point.
  \item \textbf{Approximating splines} - points act as control points, the curve does not pass through the points.
\end{itemize}

\subsection{Non-Parametric Splines}
The polynomial spline:
\[
  y = a_2 x^2 + a_1 x + a_0 
\]
We can find $a_2, a_1, a_0$ given any three points.

There is no control using parametric splines, only \textbf{one curve} fits the data.

\subsection{Parametric Splines}
In vector form:
\[
  P = a_2 \mu^2 + a_1 \mu + a_0 
\]
by convention as $\mu$ ranges from 0 to 1, the point $P$ traces out a curve.

Therefore:
\begin{align*}
  \mu &= 0 & P_0 = a_0 \\
  \mu &= \frac{1}{2} & P_1 = \frac{1}{4}a_2 + \frac{1}{2}a_1 + P_0 \\
  \mu &= 1 & P_2 = a_2 + a_1 + P_0
\end{align*}
which we can use to solve $a_1, a_2$.

\subsection{Cubic Spline Patches}
\begin{figure}[htb!]
  \centering
  \caption{Cubic spline patch.}
  \includegraphics[scale=0.5]{splinepatches}
\end{figure}
The order of the spline matches the number of knots, higher order polynomials are undesirable as they tend to oscillate - we can solve this with \textbf{spline patches}, piecing together a number of parametric splines.

We can calculate parametric spline patches using a cubic polynomial:
\[
  P = a_3 \mu^3 + a_2 \mu^2 + a_1 \mu + a_0 
\]

Then from any successive set of four control points:
\[
  \begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \end{pmatrix} =
  \begin{pmatrix}
    1 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    -3 & -2 & 3 & -1 \\
    2 & 1 & -2 & 1
  \end{pmatrix}
  \begin{pmatrix} P_i \\ P_i' \\ P_{i+1} \\ P_{i+1}' \end{pmatrix}
\]

\subsection{Continuity}
A function is smooth if its derivatives are well defined up to some order.

\subsubsection{Parametric Continuity}
We view the curve or surface as a function rather than a shape; at the junction between curves:
\begin{itemize}
  \item $C^0$ continuous if the $(x, y)$ values of the two curves agree.
  \item $C^1$ continuous if their first derivatives also agree.
  \item $C^2$ continuous if additionally their second derivatives agree.
\end{itemize}

\subsubsection{Geometric Continuity}
Defined using only the shape of the curve; at the junction between the curves:
\begin{itemize}
  \item $G^0$ continuous if the $(x, y)$ values of the two curves agree.
  \item $G^1$ is their first derivatives are also proportional (the tangent vectors are parallel).
\end{itemize}

\subsection{Bezier Curves}
Bezier curves give predictable results for small sets of knots and are useful as spline patches:
\begin{itemize}
  \item End points are interpolated.
  \item The slope at the end is the same as the line joining the end point to its neighbour.
\end{itemize}

\subsubsection{Casteljau's Algorithm}
\begin{figure}[htb!]
  \centering
  \caption{Casteljau construction with $\mu = 0.25$, the final point is $P_{3,0}$.}
  \includegraphics[scale=0.5]{casteljau}
\end{figure}

We construct the curve recursively with any value of $\mu$.

\subsection{Bernstein Blending Function}
Splines and Bezier curves can be formulated as a blend of the knots.
We can consider $P = (1 - \mu)P_0 + \mu P_1$ as the linear blend of two points, the two-point Bezier curve.

Any point on the spline is simply a blend of all the other points.
For $N + 1$ knots we have:
\[
  P(\mu) = \sum_{i=0}^N W(N, i, \mu)P_i 
\]
where $W$ is the Bernstein blending function:
\[
  W(N, i, \mu) \binom{N}{i} \mu^i (1 - \mu)^{N - i}
\]

\subsubsection{Bezier Curves and Cubic Patches}
Since all the knots of the Bezier curve appear in the blend, they cannot be used for curves with fine detail.
They are however effective as spline patches.

In fact, a four point Bezier curve is \textbf{equivalent} to a cubic patch going through the first and last knots of the curve.

We can show this by:
\begin{enumerate}
  \item First expanding the iterative Bernstein blending function to show its equivalence to the cubic patch.
  \item Then reversing the Casteljau algorithm to show equivalence to the expanded Bernstein blending function, and thus to the cubic spline patch.
\end{enumerate}

We can summarize the Bezier curve by saying it has:
\begin{itemize}
  \item Two points that are interpolated: $P_0, P_3$.
  \item Two control points: $P_1, P_2$ - can be interacted with to determine the shape.
\end{itemize}

\section{Surface Construction}
\subsection{Simple Parametric Surfaces}
\begin{figure}[htb!]
  \centering
  \caption{Curves and known points of a parametric surface.}
  \includegraphics[scale=0.5]{parametricsurface}
\end{figure}

\[
  \bm{P}(\mu, v) = \bm{a}\mu^2 + \bm{d}v^2 + 2\bm{b}\mu v + 2\bm{c} \mu + 2 \bm{e} v + \bm{f}
\]
To solve for the six unknown parameter vectors, we can substitute the $\mu, v$ values of known points.

For the edges, we notice that either $\mu$ or $v$ are $0$ or $1$ at the edges, while the other ranges from $[0, 1]$.

\subsection{Parametric Patches}
As with curves, we create a surface by joining a lot of simple surfaces continuously - we need to match three values at each corner:
\begin{align*}
  \bm{P}(\mu, v) && \frac{\partial\bm{P}(\mu, v)}{\partial \mu} && \frac{\partial\bm{P}(\mu, v)}{\partial v}
\end{align*}

\subsubsection{Edges}
\begin{figure}[htb!]
  \centering
  \caption{Spline patch.}
  \includegraphics[scale=0.5]{splinepatch}
\end{figure}
We need to ensure the patch joins its neighbours exactly at the edges - we ensure the edge contours are the same on adjacent patches.

We do this identically as with cubic spline curve patches - we ensure the gradients are the same for the four patches that meet at a point.

\subsubsection{Coons Patch}
To define the internal points, we linearly interpolate:
\begin{align*}
  \bm{P}(\mu, v) &= \bm{P}(\mu, 0)(1-v) + \bm{P}(\mu, 1)v
                 &+ \bm{P}(0, v)(1-\mu) + \bm{P}(1, v)\mu
                 &- \bm{P}(0, 1)(1-\mu)v - \bm{P}(1, 0)\mu(1-v)
                 &- \bm{P}(0,0)(1-\mu)(1-v) - \bm{P}(1,1)\mu v
\end{align*}

\subsection{Rendering a Patch}
\begin{itemize}
  \item \textbf{Polygonization} - select a grid of points and triangulate.
    \begin{itemize}
      \item Large polygons for speed.
      \item Smaller to match pixel size for accuracy.
    \end{itemize}
  \item \textbf{Ray Tracing} - numerical ray-patch algorithm:
    \begin{enumerate}
      \item Polygonize the patch at a low resolution.
      \item Calculate the ray intersection with the triangles and find the nearest intersection.
      \item Polygonize the immediate area of the intersection, calculate a better estimate of the intersection.
      \item Continue until the best estimate is found.
    \end{enumerate}
\end{itemize}

\section{Warping and Morphing}
\begin{defn}
  \textbf{Warping} is the geometric transformation of graphical objects between coordinate systems.
\end{defn}

\begin{defn}
  \textbf{Morphing} refers to an animation technique in which one graphical object is gradually turned into the other; the aim is to find the average object.
\end{defn}

\subsection{Averaging}
To find the average of two points $\bm{P}, \bm{Q}$, we use linear interpolation:
\[
  \bm{P} + 0.5\bm{v} = \bm{P} + 0.5(\bm{Q} - \bm{P}) = 0.5\bm{P} + 0.5\bm{Q} 
\]

\subsection{Image Blending}
We need to determine how to combine attributes associated with geometrical primitives; such attributes include: colour, texture coordinates, and normals.

\textbf{Cross-dissolve}:
\[
  I = (1 - t) \cdot I_A + t\cdot I_B 
\]

If the images are not aligned, we must perform warping first, $\text{Morphing } = (\text{warping})^2 + \text{ blending}$.

\subsection{Image Warping}
\begin{itemize}
  \item \textbf{Image filtering} - changing the \textbf{range} of the image, $g(x) = T(f(x))$.
  \item \textbf{Image warping} - changing the \textbf{domain} of the image, $g(x) = f(T(x))$.
\end{itemize}

\subsubsection{Forward Warping}
Given a coordinate transform $(x', y') = \bm{T}(x, y)$ and a source image $f(x,y)$, we compute the transformed image $g(x', y') = f(T(x, y))$:
\begin{enumerate}
  \item Send each pixel $f(x, y)$ to its corresponding location $(x', y')$ in the second image.
  \item If the pixel lands between two pixels, we distribute colour among the neighbouring pixels.
\end{enumerate}

\subsubsection{Inverse Warping}
To get the corresponding location for each pixel $g(x', y')$:
\begin{enumerate}
  \item $(x, y) = \bm{T}^{-1}(x', y')$.
  \item If the pixel comes from between two pixels, interpolate colour value from neighbours.
\end{enumerate}

\subsection{Feature-Based Warping: Beier-Neeley}
We use pairs of lines to specify warp:
\begin{align*}
  u = \frac{(p-x)\cdot(y-x)}{\lVert y - x \rVert^2} &&
  v = \frac{(p-x)\cdot\text{ Perpendicular}(y - x)}{\lVert y - x \rVert}
\end{align*}
$u$ is a fraction, while $v$ is a length.

For each pixel $p$ in the destination image:
\begin{enumerate}
  \item Find the corresponding $u, v$.
  \item Find the $p'$ in the source image for that $u, v$:
    \[
      p' = x + u \cdot (y' - x') + \frac{v \cdot \text{ Perpendicular}(y'-x')}{\lVert y'-x' \rVert} 
    \]
  \item 
\end{enumerate}

With a single line pair we can translate, scale, or rotate.

\subsubsection{Multiple Line Pairs}
We can warp using multiple line pairs by using the weighted combination of points defined by each pair of corresponding lines.

For each line pair:
\[
  \text{weight}[i] = \left( \frac{\text(length)[i]^p}{a + \text{ dist}[i]} \right)^b 
\]

\subsection{Warping using Control Points}
\begin{figure}[htb!]
  \centering
  \caption{Partitioning the convex hull of the control points into a set of triangles; the original points are underneath the displaced points.}
  \includegraphics[scale=0.5]{controlpoints}
\end{figure}
\begin{enumerate}
  \item Each control point has a displacement vector.
  \item Partition the convex hull of the control points into a set of triangles.
  \item Find the triangle which contains point $\bm{p}$, express it in terms of the vertices of the triangle:
    \[
      \bm{p} = \bm{x}_1 + \alpha(\bm{x}_2 - \bm{x}_1) + \beta(\bm{x}_3 - \bm{x}_1)
    \]
  \item Under the affine transformation:
    \[
      \bm{p}' = \gamma\bm{x}_1' + \alpha\bm{x}_2' + \beta\bm{x}_3' 
    \]
    where $\gamma = 1 - (\alpha + \beta)$.
\end{enumerate}
This can result in non-smooth transformations as straight lines may be kinked across triangle boundaries.

\subsubsection{Triangulations}
\begin{defn}
  A \textbf{triangulation} of a set of points in the plane is a partition of the convex hull to triangles, whose vertices are the points and do not contain other points.
\end{defn}

For quality triangulations, we want to maximize smallest angles.

\section{Radiosity}
In ray tracing, we assumed a small number of point light sources.
In the reflectance equation, every surface reflects light so each should be considered a light source - we must refine the ambient light.

\subsection{Ambient Light}
We make ambient light a function of the incident light as well:
\[
  I_{reflected} = k_a I_{incident} + k_d(\bm{n} \cdot \bm{l}) I_{incident} + k_s(\bm{r} \cdot \bm{v})^q I_{incident} 
\]
or more simply $I_{reflected} = R I_{incident}$, where $R$ is the viewpoint dependent reflectance function.

\subsection{Radiosity}
We divide the scene into patches $i = 1, \dots, n$, for the $i$-th patch:
\[
  B_i = E_i + R_i I_i 
\]
where:
\begin{itemize}
  \item $B_i$ is the total energy leaving.
  \item $E_i$ is the total energy emitted.
  \item $R_i$ is the reflectance value.
  \item $I_i$ is the incident energy arriving.
\end{itemize}

We can estimate the incident energy as $I_i = \sum_{j = 1}^n B_j F_{ij}$, where $F_{ij}$ is the \textbf{form factor} - the link between patch $i$ and $j$, taking into account the angle they face each other.

\[
  B_i = E_i + R_i \sum_j B_j F_{ij} 
\]
Solving this using a matrix is very computationally intense.

The radiosity values are wavelength dependent so separate values must be computed for $R, G, B$.

\subsection{Specular Reflections}
The specular component depends on the relative positions of the viewpoint and light.
Every patch is a light source, and each light source is no longer a point - this is computationally infeasible.

\subsection{Large Polygons}
When dividing into patches for small polygons, we can use the polygon map.
For large polygons, we need to subdivide them:
\begin{itemize}
  \item Emitted light is not constant across a large polygon -  we will see subdivisions.
  \item We can project patches to sub-pixel size, then smooth the results.
\end{itemize}

\subsection{Form Factors}
These couple two patches and determine the proportion of radiated energy from one that strikes the other - no coupling when in the same plane, maximal coupling when they directly face each other.

\[
  F_{ij} = \frac{\cos \phi_i \cos \phi_i \lvert A_j \rvert}{\pi r^2} 
\]
where:
\begin{itemize}
  \item $\phi_i, \phi_j$ are the angles between the normal of that patch and the line to the other patch.
  \item $A_j$ is the area of patch $j$.
  \item $r$ is the distance between the patches.
\end{itemize}

\subsection{Hemicube Method}
It can be shown that all patches that project onto the same area of a hemisphere have the same form factor - we use hemicubes as they are less computationally intense.

The hemicube is divided into small pixel areas and form factors are computed for each pixel.
The factors for each pixel will be the same for every patch so we can store them in a look-up table, then look-up the ones which light up.
The (\textbf{delta form factor}) computation can be simplified:
\begin{itemize}
  \item Top faces: $\frac{\lvert A \rvert}{\pi r^4}$.
  \item Side faces: $\frac{z_p \lvert A \rvert}{\pi r^4}$.
\end{itemize}
Additionally $F_{ji} = \frac{F_{ij}\lvert A_i \rvert}{\lvert A_j \rvert}$, so only half the patches need to be computed.

We can use ray tracing to determine the patches that are actual visible from each hemicube pixel.

Finally, we sum the delta form factors to get the final form factor.

\subsection{Gauss-Seidel Method}
To solve the matrix, we can use the \textbf{Gauss-Seidel} method to approximate, by using the iteration:
\[
  B_i^k = E_i + R_i \sum_j B_j^{k - 1}F_{ij} 
\]
to give successive estimates, with initial values $B_i^0$.

\subsubsection{Shooting and Gathering}
\begin{itemize}
  \item \textbf{Gathering} is the evaluation of one $B_i$ value.
  \item \textbf{Shooting} is the change to every other path from a change in one patch, through $\Delta B_i$.
\end{itemize}

We choose to evaluate the patch with the largest change $\Delta B$ first, then shoot the radiosity to all other patches - this leads to the fastest convergence.

\end{document}
