\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{amsmath} \usepackage{amssymb}
\usepackage{graphicx}

\graphicspath{ {./imgs/} }

\setlength{\parindent}{0pt}

\title{Performance Engineering (CO339)}
\author{Michael Tsang}

\newtheorem{defn}{Definition}
\newtheorem{eg}{Example}
\newtheorem{theo}{Theorem}
\newtheorem{lem}{Lemma}

\begin{document}

\maketitle

\section{Motivation}
\textit{Performance engineering encompasses the techniques applied during a systems development life cycle to ensure the \textbf{non-functional requirements for performance} will be met.}

\subsection{System}
\begin{defn}
  A regularly interacting or independent group of items forming a unified whole.
\end{defn}

\begin{itemize}
  \item Made up from \textbf{components} that interact to achieve a \textbf{greater goal}.
  \item Applicable to many situations (generic).
  \item Goal is domain-agnostic.
  \item Components serve a data-management purpose, not a domain purpose.
\end{itemize}

\subsection{Success}
To define success, we need to define a \textbf{metric} and a \textbf{threshold}.

The metric could be throughput, latency, scalability, memory usage, energy consumption, TCO, elasticity, efficiency\dots.

We have two options when declaring success:
\begin{itemize}
  \item Setting an optimization \textbf{budget}.
  \item Setting an optimization \textbf{target}.
\end{itemize}

\subsection{Defining the Target}

\subsubsection{\textit{SMART} Requirements}
\begin{itemize}
  \item \textbf{Specific} - state exactly what is acceptable in numeric terms.
  \item \textbf{Measureable} - make sure what is stated can be measured.
  \item \textbf{Acceptable} - rigorous enough to guarantee success in reality.
  \item \textbf{Realizable} - lenient enough to allow implementation.
  \item \textbf{Thorough} - all necessary aspects of the system are specified.
\end{itemize}

\subsubsection{Quality-of-Service (QoS) Objectives}
\begin{defn}
  Statistical properties of a metric that shall hold for the system.
\end{defn}
\begin{itemize}
  \item Can include pre-conditions.
  \item Could conflict with functional requirements.
\end{itemize}

\begin{eg}
  The framerate of the game will, on average, be higher than 60 frames/s if run on a GPU with 50 GFlops or more.
\end{eg}

\subsubsection{Service-Level Agreements (SLA)}
\begin{defn}
  Formal, legal contracts specifying QoS objectives, as well as penalities for violations.
\end{defn}
\begin{itemize}
  \item Needs enforcement through monitoring.
\end{itemize}

\begin{eg}
  Trading orders shall not exceed 1ms response time. 
  In case of violation, the user is eligible for a 10\% credit towards fees.
\end{eg}


\subsubsection{Monitoring}
\begin{itemize}
  \item Constant monitoring is required to enforce SLAs.
    \begin{itemize}
      \item Observe system performance.
      \item Collect statistics.
      \item Analyze data.
      \item Report SLA violations.
    \end{itemize}
  \item Monitoring can incur costs $\rightarrow$ often not continuous.
\end{itemize}

\subsection{Fulfilling Performance Requirements}
Examples of performance evaluation techniques are:
\begin{itemize}
  \item Simulation.
  \item Measuring.
  \item Analytical modeling.
  \item Hybrids, e.g.\ measure then model; model and simulate\dots
\end{itemize}

\subsubsection{Measuring}
\begin{itemize}
  \item Performed on a prototype or on the final system.
  \item Closely related to monitoring, promises good accuracy.
  \item Often based on instrumentation.
  \item Costly, and can be difficult.
\end{itemize}

\subsubsection{Benchmark}
Two step process:
\begin{enumerate}
  \item Get the system into a predefined state.
  \item Perform a series of operations (the workload) while measuring performance.
\end{enumerate}

\subsubsection{Workloads}
\begin{itemize}
  \item \textbf{Batch} workloads:
    \begin{itemize}
      \item Program has access to entire batch at start.
      \item Useful when metric is throughput.
      \item Simple, generator performance does not matter.
    \end{itemize}
  \item \textbf{Interactive} workloads:
    \begin{itemize}
      \item Work generated piece by piece (randomly).
      \item Useful when metric is latency.
      \item Generator should be at least as fast as system under evaluation.
    \end{itemize}
  \item \textbf{Hybrids}:
    \begin{itemize}
      \item Sample random queries from a predefined work set.
    \end{itemize}
\end{itemize}

\subsubsection{Parameters}
\begin{itemize}
  \item \textbf{System Parameters} - generally do not change (e.g.\ caches; CPU instruction costs).
  \item \textbf{Workload Parameters} - may change, even while system is running (e.g.\ users; available memory).
\end{itemize}

\subsection{Interpreting Performance Metrics}
\begin{itemize}
  \item A single number is generally meaningless, there is too much noise in modern computer systems.
  \item We aggregate multiple runs and report some measure of variance - \textbf{statistics}.
  \item We can represent statistics with charts, e.g.\ box and whisker charts.
\end{itemize}

\begin{defn}
  \textbf{Utilization} is the percentage of a resource that is used to perform a service.
\end{defn}

\begin{defn}
  \textbf A {bottleneck} is the resource with the highest utilization.
\end{defn}

\subsection{Improving Performance}
We compare alternative designs (\textbf{development}) or select a close-to-optimal value for a parameter (\textbf{tuning}).

\subsubsection{Analytical Modeling}
\begin{defn}
  A set of equations describing the mathematical relationship between performance parameters and performance metrics.
\end{defn}

We model a dynamic system using \textbf{static equations}.
This is the key distinction to simulation, which is dynamic.

Analytical models are fast and allow what-if analysis, which simplifies tuning.
If we can build an analytical model, we have understood the system.

\subsubsection{Parameter Tuning}
Workload parameters are not usually under our control.
\begin{defn}
  \textbf{Parameter tuning} involves finding the vector in the parameter space that either \textbf{minimizes the resource consumption} or \textbf{maximizes a performance metric}. 
\end{defn}
We need to explore the parameter space, which is expensive.
Analytical models help to accelerate this process immensely.

\subsubsection{Tradeoffs}
Sometimes consumption of an expensive or non-scalable resource can be reduced by using more of a cheaper one - this may require changes to the system.

\section{Performance Profiling and Tracing}
\begin{defn}
  A \textbf{profile} is a graphical or other representation of information relating to particular chracteristics of something, recorded in quantified form.
\end{defn}
We characterize the system in terms of the time it spends in certain states.

The system can be applied to any level: CPU, OS, distributed, application, etc.

We profile in order to:
\begin{itemize}
  \item Identify the critical path.
  \item Identify bottlenecks.
  \item Tune.
\end{itemize}

\subsection{Events}
An \textbf{event} is some change of the system state, this depends on what needs to be measured.
There are two types:
\begin{itemize}
  \item \textbf{Simple/atomic events} - a package sent, instruction executed, address loaded from memory, a clock tick, etc.
  \item \textbf{Complex events} - cache line evicted from L1 to L2 cache, instruction aborted due to misspeculation.
\end{itemize}

\subsubsection{Collecting Events}
\begin{itemize}
  \item \textbf{Event-based/event-counting}:
    \begin{itemize}
      \item Count events on occurence.
      \item Little overhead, limited information.
    \end{itemize}
  \item \textbf{Tracing}:
    \begin{itemize}
      \item Keep state for every event.
      \item Higher overhead, more information.
    \end{itemize}
  \item \textbf{Sampling}:
    \begin{itemize}
      \item Keep state in \textbf{intervals}.
      \item Inaccurate and non-deterministic.
      \item Interval size $1$ makes sampling equal to event-based profiling.
    \end{itemize}
  \item (\textbf{Indirect}:)
    \begin{itemize}
      \item Counting events that dominate (correlate with) others (if this event occurs, then other events will occur - we consider the \textit{other events}).
      \item Can be used to reduce overhead.
      \item Accuracy depends on the event and indirection.
    \end{itemize}
\end{itemize}

\subsection{Profile}
An aggregrate over the events of a specific metric:
\begin{itemize}
  \item \textbf{Global aggregate} - total cache misses, total CPU cycles, etc.
  \item \textbf{Broken down} by some other event - cycles per instruction, cache misses by line of code.
\end{itemize}

\subsection{Trace}
A complete log of every state the system has ever been in (in a period of interest):
\begin{itemize}
  \item Characterized by the events.
  \item There is a total order of events.
  \item \textit{Stream of events}.
\end{itemize}
By aggregating, we can turn this into a profile.

\subsection{Classification}
\begin{itemize}
  \item Sampling rate (interval) $=1$:
    \begin{itemize}
      \item With payload - \textbf{tracing}.
      \item Without payload - \textbf{event-based profiling}.
    \end{itemize}
  \item Sampling rate (interval) $>1$:
  \begin{itemize}
    \item \textbf{Sampling}.
  \end{itemize}
\end{itemize}

\subsection{Collection Framework}
Our framework consists of two components:
\begin{itemize}
  \item \textbf{Generator} - produces events; usually online, part of the runtime environment.
  \item \textbf{Consumer} - producing visualisation or profile from trace; offline or online.
\end{itemize}

\subsection{Time}
\begin{itemize}
  \item Computer clocks are not very accurate.
  \item We use reference cycles as a proxy metric.
  \item Any event may be used to define intervals.
\end{itemize}

\subsection{Quantization Errors}
\begin{itemize}
  \item The interval resolution is limited.
  \item However, time is continuous.
  \item This introduces errors as costs may be attributed to the wrong instruction.
\end{itemize}

\subsection{Call Stack}
\subsubsection{Call Stack Tracing}
Recording the entire call stack is quite expensive:
\begin{itemize}
  \item Stack needs to be walked, pointers need chasing.
  \item Call stacks can be deep.
  \item Record must be written to memory.
\end{itemize}

\subsubsection{Call Stack Sampling}
\begin{itemize}
  \item Limited accuracy, but better performance.
  \item Less pertubation.
\end{itemize}

\subsubsection{Flame Graphs}
\begin{figure}[htb!]
  \centering
  \caption{A flame graph.}
  \includegraphics[scale=0.4]{flamegraph}
\end{figure}
The $x$-axis shows the stack profile population, sorted alphabetically - it does not show the passage of time.
The $y$-axis shows the stack depth, counting from zero at the bottom.

Each rectangle is a stack frame; the wider the frame, the more often it was present in the stacks.
The top edge shows what is on-CPU, beneath is its ancestry.

\subsection{Event Sources}
We require event sources to be:
\begin{itemize}
  \item \textbf{Detailed} - as much information as we need.
  \item \textbf{Accurate} - reliable measurements.
  \item \textbf{Little perturbation} - does not change the performance of the system during analysis (often inversely proportional to accuracy).
\end{itemize}

We can get events from:
\begin{itemize}
  \item \textbf{Software} - e.g.\ OS kernel counts, library logging, compiler instrumentation.
  \item \textbf{Hardware} - e.g.\ performance counter.
  \item \textbf{Emulator} - hybrid of software and hardware; minimal perturbation but not scalable.
\end{itemize}

\subsection{Instrumentation}
We augment the program with event logging code.
\begin{itemize}
  \item Advantages:
    \begin{itemize}
      \item No need for hardware support.
      \item Flexible.
    \end{itemize}
  \item Disadvantages:
    \begin{itemize}
      \item High overhead.
      \item High perturbation.
    \end{itemize}
\end{itemize}
We need to decide on what events we should collect.

There are three approaches to instrumentation:
\begin{itemize}
  \item Manual instrumentation.
  \item Automatic source-level instrumentation.
  \item \textbf{Automatic binary instrumentation} - could be static (compile-time), dynamic (runtime), or as a hybrid.
\end{itemize}

\subsubsection{Manual Instrumentation}
\texttt{printf} logging.
\begin{itemize}
  \item Advantages:
    \begin{itemize}
      \item Fine control.
      \item No need for support from hardware or compiler.
    \end{itemize}
  \item Disadvantages:
    \begin{itemize}
      \item High overhead.
      \item Disabled for release build.
    \end{itemize}
\end{itemize}

\subsubsection{Automatic Instrumentation}
Usually supported by the compiler; source-to-source rewriting is possible.
\begin{itemize}
  \item Advantages:
    \begin{itemize}
      \item Delegating expertise of what needs to be logged to the compiler.
    \end{itemize}
  \item Disadvantages:
    \begin{itemize}
      \item Less control.
      \item Need for compiler support.
    \end{itemize}
\end{itemize}

\subsubsection{Binary Instrumentation}
\begin{itemize}
  \item \textbf{Static}:
    \begin{itemize}
      \item No magic; simple and portable.
      \item Instrumentation overhead can be accessed from binary.
    \end{itemize}
  \item \textbf{Dynamic}:
    \begin{itemize}
      \item No recompilation.
      \item Can be performed on a running process.
      \item Works with JiT-code.
    \end{itemize}
\end{itemize}

\subsection{Performance Counting}
\begin{itemize}
  \item \textbf{Software Performance Counters} (OS):
    \begin{itemize}
      \item e.g.\ network packages sent, virtual memory operations.
      \item Only of secondary interest.
    \end{itemize}
  \item \textbf{Hardware Performance Counters}:
    \begin{itemize}
      \item Fixed number can be active at runtime.
      \item Usually buggy, unmaintained, and poorly documented, with poor accuracy.
      \item Common counters usually okay.
    \end{itemize}
\end{itemize}

\end{document}
